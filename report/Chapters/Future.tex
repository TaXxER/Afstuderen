Follow-up research from this research can be categorised into three categories. These categories of potential future work are described in the sections below.

\section{Distributed Computing Models}
We explored the possibilities of Hadoop MapReduce for distributed computation of Learning to Rank training algorithms. Hadoop, since the introduction of Hadoop YARN in Hadoop 2.0, offers integration with other distributed computing models, including Dryad \cite{Isard2007}, Spark \cite{Zaharia2010}, Storm \cite{Aniello2013} and \ac{MPI}. Of these distributed programming models, Spark is particularly promising, since Shukla et al. \cite{Shukla2012} already showed that it is able to speed-up the ListNet training procedure with much lower job scheduling overhead than that we found for MapReduce. For the newly supported programming models in Hadoop, it holds that they lack the critical mass as the distributed programming model of choice. This lack of a critical mass results in higher integration costs and less available support. The integration of new programming model into the Hadoop YARN framework is a step in the good direction to alleviate the integration cost of these programming models, but does not yet completely eliminate the problem. Cloud-based Hadoop services like Microsoft HDInsight and Amazon Elastic MapReduce.

\section{Learning to Rank Algorithms}
As an answer to the first research question of this thesis, we found five Learning to Rank methods for it holds that no other algorithm produced more accurate rankings with a higher degree of certainty of ranking accuracy. These algorithms were, from highest ranking accuracy / lowest certainty to lowest ranking accuracy / highest certainty: ListNet, SmoothRank, FenchelRank, FSMRank and LRUF. In this thesis we explored the speed-up characteristics of ListNet on Hadoop MapReduce. The speed-up characteristics of SmoothRank, FenchelRank, FSMRank and LRUF are using the MapReduce computing model are still to be explored.

\section{Optimisation Algorithms}
The gradient descent optimisation procedure can often be replaced with other optimisation procedures with faster convergence properties, often at the cost of being more computationally expensive per iteration. Lin \cite{Lin2013} hypothesised that the replacement of gradient descent by optimisation methods with faster convergence properties can especially be beneficial in a MapReduce setting, as the MapReduce job scheduling overhead leads to a high constant factor in iteration time. An example of an optimisation procedures that fits this higher convergence rate at the cost of higher per-iteration computational costs is L-BFGS \cite{Liu1989}, or any other quasi-Newton optimisation method. Testing the effect that replacing the optimisation method of Learning to Rank methods has on the speed-up that can be achieved by parallelising the methods with the MapReduce model is still to be determined. Note however that replacing the optimisation method in a Learning to Rank algorithm basically turns it into a different, new, Learning to Rank method, as different optimisation methods might not be equivalent in how well they are able to find a good set of model parameters. One should therefore also explore the ranking accuracy characteristics when an existing Learning to Rank method is evaluated in combination with an optimisation algorithm that is not prescribed to be a part of the Learning to Rank method.