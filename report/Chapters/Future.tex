\section{Computing models}
We explored the possibilities of Hadoop MapReduce for distributed computation of Learning to Rank training algorithms. Hadoop, since the introduction of Hadoop YARN in Hadoop 2.0, offers integration with other distributed computing models, including Dryad \cite{Isard2007}, Spark \cite{Zaharia2010}, Storm \cite{Aniello2013} and MPI. Of these distributed programming models, Spark is particularly promising, since Shukla et al. \cite{Shukla2012} already showed that it is able to speed-up the ListNet training procedure with much lower job scheduling overhead than that we found for MapReduce. For the newly supported programming models in Hadoop, it holds that they lack the critical mass as the distributed programming model of choice. This lack of a critical mass results in higher integration costs and less available support. The integration of new programming model into the Hadoop YARN framework is a step in the good direction to alleviate the integration cost of these programming models, but does not yet completely eliminate the problem. Cloud-based Hadoop services like Microsoft HDInsight and Amazon Elastic MapReduce.

\section{Learning to Rank algorithms}