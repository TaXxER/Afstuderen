Algorithms and details of the well-performing Learning-to-Rank methods as selected in the afore-going part are presented and explained in this chapter.

\section{ListNet}
ListNet \cite{Cao2007} is a listwise ranking function whose loss function is not directly related to information retrieval evaluation metrics. ListNet's loss function is defined using a probability distribution on permutations. Probability distributions on permutations have been a research topic within the field of probability theory and has been extensively researched. One of the most well-known permutation probability distributions in the field, the Plackett-Luce model \cite{Plackett1975,Luce1959}, is used in ListNet. The Plackett-Luce model defines the probability of all possible permutations $\pi$, given all document ranking scores $S$, as shown in Equation \ref{eq:plackett_luce}.
\begin{equation}
P(\pi|S) = \prod\limits_{j=1}^{m}\frac{\phi(s_{\pi^{-1}(j)})}{\sum\nolimits_{u=1}^{m}\phi(s_{\pi^{-1}(u)})}
\label{eq:plackett_luce}
\end{equation}
ListNet uses Gradient Descent to optimise neural network such that its Cross Entropy loss compared to the Plackett-Luce distribution over the ground truth is minimal. Note that some sources, including Liu \cite{Liu2007}, describe ListNet as using KL divergence as loss function. KL divergence and Cross-entropy are however identical up to an additive constant when comparing distribution $q$ against a fixed reference distribution $p$, since the cross-entropy definition can be formulated as in Equation \ref{eq:cross_entropy}.
\begin{equation}
H(p,q) = H(p) + D_{KL}(p||q)
\label{eq:cross_entropy}
\end{equation}
where $H(p)$ is the entropy of $p$ and $D_{KL}(p||q)$ is the Kullback-Leibler divergence of $q$ from $p$.\\

\noindent Equation \ref{eq:gradient_descent} describes the gradient descent step to minimise loss function $L(y^{(i)},z^{(i)}(f_\omega))$ with respect to parameter $\omega$. 
\begin{equation}
\Delta\omega = \frac{\partial L(y^{(i)},z^{(i)}(f_\omega))}{\partial \omega} = - \sum_{\forall g \in \mathscr{G}_k}\limits\frac{\partial P_{z^{(i)}(f_\omega)}(g)}{\partial \omega}\frac{P_{y^{(i)}}(g)}{P_{z^{(i)}(f_\omega)}(g)}
\label{eq:gradient_descent}
\end{equation}

\noindent Algorithm \ref{alg:listnet} shows the pseudo-code of the ListNet training phase.\\
\LinesNumbered
\begin{algorithm}[H]
 \KwData{training data \{$(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),...,(x^{(m)},y^{(m)})$\}}
 \KwIn{number of iterations $T$ and learning rate $\eta$}
 Initialize parameter $\omega$\\
 \For{$t\leftarrow 1$ \KwTo $T$}{
 	\For{$i\leftarrow 1$ \KwTo $m$}{
 		Input $x^{(i)}$ of query $q^{(i)}$ to Neural Network and compute score list $z^{(i)}(f_\omega)$ with current $\omega$.\\
 		Compute gradient $\Delta\omega$ using Eq. (\ref{eq:gradient_descent}).\\
 		Update $\omega = \omega - \eta \times \Delta\omega$.
 	}
 }
 Output Neural Network model $\omega$.
 \caption{Learning algorithm of ListNet, obtained from \cite{Cao2007}}
 \label{alg:listnet}
\end{algorithm}

\section{SmoothRank}
SmoothRank \cite{Chapelle2010} is a listwise ranking method that directly optimises an information retrieval evaluation measure by smoothing, that is, approximating the rank position. In this section I will use \ac{nDCG} for illustration of SmoothRank, but the same procedure can also be applied to \ac{MAP} or any other information retrieval measure. The smoothing function used in SmoothRank based on the softmax activation function \cite{Bridle1990} that is often used in neural networks, which is formally defined as shown in Equation \ref{eq:softmax_algorithm}.
\begin{equation}
p_i = \frac{e^{f_i/\sigma}}{\sum\nolimits_{j=1}^{m}e^{f_j/\sigma}}
\label{eq:softmax_algorithm}
\end{equation}

where $\sigma$ is the smoothing parameter. Chapelle et al \cite{Chapelle2010} apply the softmax activation function to the \ac{nDCG} formula and thereby introduce a soft version of indicator variable $h_{i,j}$ as formulated in Equation \ref{eq:soft_ndcg}.

\begin{equation}
h_{i,j} = e^{-\frac{(f(x_i))-(f(x_{d(j)}))^2}{\sigma}}\Big/\sum\nolimits_{k=1}^{m}e^{-\frac{(f(x_k))-f(x_{d(j)})^2}{\sigma}}
\label{eq:soft_ndcg}
\end{equation}

It can be shown that the derivative of the smoothed \ac{nDCG} version shown in Equation \ref{eq:soft_ndcg} and the smoothed versions of other \ac{IR} metrics can be calculated in $\mathcal{O}(m^2)$, which enable fast gradient descent optimisation. The optimisation step in SmoothRank uses the nonlinear conjugate gradient method  with Polak-Ribiere update \cite{Shewchuk1994}, which is a type of gradient descent method. The optimisation method is prone to local optima, which is alleviated by adding a good starting point and regulariser.\\

The starting point in SmoothRank is set to either the solution of a simple linear regression, or alternatively to the solution of Rank\acs{SVM}. Since this starting point is expected to already be a good solution, a regulariser term is added to the SmoothRank objective function to prevent the solution from deviating to much from the starting point. The regularised smooth objective function is formulated as $\lambda||w-w_0||^2$ where $\lambda$ is a hyperparameter tuned on the validation set, and $w_0$ is the starting point solution.\\

The choice of the smoothing parameter $\sigma$ in Equation \ref{eq:soft_ndcg} is important, because a too small value makes the function more non-smooth and therefore harder to optimise, while a too large value results in a optimisation function that substantially differs from the optimal rankings. SmoothRank uses an annealing method where the optimisation procedure starts with a large $\sigma$ and iteratively reduces it by dividing it by 2 at each step. Algorithm \ref{alg:smoothrank} shows the algorithm that summarises all steps of the SmoothRank method.\\
\LinesNumbered
\begin{algorithm}[H]
 Find an initial solution $w_0$ (by regression or Rank\acs{SVM}).\\
 Set $w = w_0$ and $\sigma$ to a large value.\\
 \While{Stopping condition not satisfied}{
 	 Starting from $w$, minimize by non-linear conjugate gradient descent:
 	 $\lambda||w-w_0||^2 - \sum\nolimits_{q}A_q(w,\sigma)$\\
 	 $\sigma = \sigma/2$
 }
 \caption{Learning algorithm of SmoothRank, obtained from \cite{Chapelle2010}}
 \label{alg:smoothrank}
\end{algorithm}

\section{FenchelRank}
FenchelRank \cite{Lai2013} is a ranking method that addresses the sparse Learning-to-Rank problem, which is the problem of learning a ranking function with only a few non-zero coefficients with respect to the input features. FenchelRank is based on the theory of Fenchel Duality \cite{Rifkin2007} and uses a generic algorithm framework proposed by Shalev-Shwartz and Singer \cite{Shalev-Shwartz2010}. FenchelRank optimises the objective function shown in Equation \ref{eq:pairwise_l1_loss} which is equivalent to the standard $\ell_1$-norm regularised pairwise loss function. The $\ell_1$-norm regularisation function in this equation is represented by $||x||_1$ and is defined as $||x||_1=\sum\nolimits_i|x_i|$.
\begin{equation}
\min_w G(w) = \min_w I_{||w||_{1} \le 1}(w) + \frac{r^2}{p} \sum\limits_{i=1}^{p}\max(0,\frac{1}{r}-(Kw)_i)^2
\label{eq:pairwise_l1_loss}
\end{equation}

\noindent where $I_{C}(w)$ is a function that is 0 if condition $C$ is satisfied, and $\infty$ otherwise. $m$ is the dimension of the data, $p$ is the number of comparable object pairs, $K$ is a matrix in $\mathbb{R}^{p \times m}$ that contains pairwise information. The objective function in Equation \ref{eq:pairwise_l1_loss} is not differentiable everywhere because of its $\ell_1$-norm regularisation term. Fenchel's duality theorem \cite{Rifkin2007}, defined in Equation \ref{eq:fenchel_duality_theorem}, provides a way to approximate the optimal value of this non-differentiable optimisation problem by instead solving the Fenchel dual of the optimisation problem.
\begin{equation}
\min_x(f(x)-g(x)) = \max_p(f_*(p)-f^*(p))
\label{eq:fenchel_duality_theorem}
\end{equation}
\noindent where $f^*$ is the convex conjugate of $f$ and $g_*$ is the concave conjugate of $g$.

\noindent Let $D(w) = -G(w)$. This allows the FenchelRank objective function (Equation \ref{eq:pairwise_l1_loss}) to be rewritten using Fenchel's duality theorem (Equation \ref{eq:fenchel_duality_theorem}) to the form shown in Equation \ref{eq:fenchel_dual}.
\begin{equation}
\max_w D(w) = \max_w I_{||w||_{1} \le 1}(w) - \frac{r^2}{p} \sum\limits_{i=1}^{p}\max(0,\frac{1}{r}-(Kw)_i)^2
\label{eq:fenchel_dual}
\end{equation}

Algorithm \ref{alg:fenchelrank} shows the FenchelRank training algorithm to optimise the Fenchel dual of the pairwise loss function. The $||x||_infty$ term in this algorithm represents an $\ell_\infty$-norm regularisation term and is defined as $||x||_\infty=\max_{i}|x_i|$.\\

\LinesNumbered
\begin{algorithm}[H]
 \KwData{pairwise training data matrix $K$}
 \KwIn{desired accuracy $\epsilon$, maximum number of iterations $T$ and the radius $r$ of the $\ell1$ ball.}
 Initialize: $w_1$ = $0_m$\\
 \For{$t\leftarrow 1$ \KwTo $T$}{
 	//check if the early stopping criterion is satisfied\\
 	\If{$||g_{t}||_{\infty} + \langle d_{t}, -Kw_{t} \rangle \le \epsilon$}{
 	//here $d_{t} = \nabla f^{*}(-Kw_{t}) = \frac{\partial f^{*}(-Kw)}{\partial(Kw)}|w=w_{t}$, \\
 	// $\langle x,y \rangle$ represents the inner products of vectors $x$ and $y$, and\\
 	// $g_{t} = d_{t}^{T}K$\\
 	return $w_{t}$ as ranking predictor $w$
 	}
 //greedily choose a feature to update\\
 Choose $j_{t} = \argmax_{j}|(g_t)_j)|$\\
 //compute an appropriate step size\\
 Let $\mu_t = \argmax_{0 \le \mu_{t} \le 1} D((1-\mu_{t})w_{t} + \mu_{t}\sign((g_{t})_{j_{t}})e^{j_{t}})$\\
 //update the model with the chosen feature and step size\\
 Update $w_{t+1}=(1-\mu_{t})w_{t} + \mu_{t}\sign((g_{t})j_{t})e^{j_{t}}$
 }
 return $w_{T}$ as ranking predictor for $w$
 \caption{Learning algorithm of FenchelRank, obtained from \cite{Lai2013}}
 \label{alg:fenchelrank}
\end{algorithm}
\section{FSMRank}
Lai et al. \cite{Lai2013c} observed that existing feature selection methods in Learning-to-Rank all follow a two-stage paradigm: 1) select subset of features from the original features, and 2) learn a ranking model based on the selected features. It is stated as a limitation of this paradigm that the selected features in the first step are not necessarily the optimal features for the second stage where the ranking model is build. FSMRank \cite{Lai2013c} addresses this limitation by using a joint convex optimisation problem that minimises ranking errors while simultaneously selecting a subset of features.\\

FSMRank uses an extended version of gradient descent optimisation, proposed by Yuri Nesterov, that enables faster convergence for convex problems \cite{Nesterov2004}. Nesterov's accelerated gradient descent can guarantee an $\epsilon$-accurate solution in at most $T$ iterations where $\epsilon=\mathcal{O}(1/T^2)$.\\

Let $S = {(q_k, \hat{X}^{(q_k), Y^{(q_k)}})}_{k=1}^n$ be a training set with queries $q_k$, corresponding retrieval objects $\hat{X}^{(q_k)}$, and corresponding relevance labels $Y^{(q_k)}$. Let $||x||_1$ be the $l_1$-norm regularisation function that is defined as $||x||_1 = \sum\nolimits_i |x_i|$. Let $\oslash$ be the element-wise division operator. The convex joint optimisation function in FSMRank is defined as shown in Equation \ref{eq:fsmrank_optimisation_function}.
\begin{equation}
\min_{\hat{w}} \frac{\lambda_1}{2} \hat{w}^T \hat{A}\hat{w} + \lambda_2 ||\hat{w}\oslash\hat{s}||_1 + f(\hat{w}, (\hat{X}^{(q_k)}, Y^{(q_k)})_{k=1}^n)
\label{eq:fsmrank_optimisation_function}
\end{equation}
\noindent $\hat{A}$ is a $d \times d$ similarity matrix that contains similarity scores between features. The first term in Equation \ref{eq:fsmrank_optimisation_function} applies a penalty on redundancy of large weighted features by using the $\hat{A}$ matrix. The well-known Pearson correlation coefficient is used to calculate similarity matrix $\hat{A}$ based on the values for the features in the training data. $\lambda_1$ is a hyper-parameter of the model and can be used to set the weight of the similarity penalty term.\\

The second term of Equation \ref{eq:fsmrank_optimisation_function} uses a $\ell_1$-norm regularisation term to select the effective features from the feature set. $\lambda_2$ is a hyper-parameter of the model and can be used to set the weight of the regularisation term.\\

The last term represents the loss function in terms of ranking errors. The loss function $f$ can in theory be any convex loss function, but Lai et al. \cite{Lai2013c} used a squared hinge loss function for their evaluation measurements. 

Algorithm \ref{alg:fsmrank} shows the steps of the FSMRank algorithm. In this algorithm an extended version of Nesterov's accelerated gradient method is used that can handle optimisation problems in the form of $\min_w \l(w) + r(w)$ where $l(w)$ is a convex function with Lipschitz gradient and $r(w)$ is convex, but non-smooth. The optimization function of Equation \ref{eq:fsmrank_optimisation_function} is reformulated to match this form as presented in Equation \ref{eq:fsmrank_optimisation_reformulated}.
\begin{equation}
\argmin_{w_t:w_t\in \mathbb{R}} Q(w_t,z_t) = \lambda_2 \sum\nolimits_{i=1}^{2d}\frac{w_i}{s_i}+\langle l^{'}(z_t),w_t-z_t \rangle+\frac{L}{2}||w_t-z_t||^2
\label{eq:fsmrank_optimisation_reformulated}
\end{equation}
\noindent where $Q(w_t,z_t)$ is a combination of the non-smooth part $r(w_t)$ and a quadratic approximation of the smooth part $l(w_t)$. $lambda_1$, $lambda_2$ and Lipschitz constant $L_0$ are input parameters of the algorithm and can be optimised through cross-validation or on a validation set.\\

\LinesNumbered
\begin{algorithm}[H]
 \KwData{training set $S = {(q_k,\hat{X}^{(q_k)},Y^(q_k)}_{k=1}^{n}$}
 \KwIn{$\lambda_1, \lambda_2, T \text{ and } L_0$}
 Initialize: $w_0=z_1=0, \alpha_1=1, \gamma=2, L=L_0/\gamma^{10}$\\
 \For{$t\leftarrow 1$ \KwTo $T$}{
 	Let $g = l^{'}(z_t)$\\
 	\While{true}{
 		//projection step\\
 		$w_t= \argmin_{w_t:w_t\in \mathbb{R}_{+}^{2d}} Q(w_t,z_t)$\\
 		\If{$l(w_t) \le l(z_t) + \langle l^{'}(z_t),w_t-z_t \rangle +\frac{L}{2}||w_t-z_t||^2$}{
 			break\\
 		}
 		$L = \lambda L$ 		
 	}
 	\If{$\frac{|F(w_t)-F(w_{t-1})|}{|F(w_{t-1})|} \le \epsilon_s$}{
 		//early stopping criterion\\
 		break
 	}
 	$\alpha_{t+1}=\frac{1+\sqrt{1+4\alpha_t^2}}{2}$\\
 	$z_{t+1} = w_t + \frac{\alpha_t-1}{\alpha_t+1}(w_t-w_{t-1})$ 
 }
 
 \caption{Learning algorithm of FSMRank, obtained from \cite{Lai2013c}}
 \label{alg:fsmrank}
\end{algorithm}

\section{LRUF}
Learning automaton.
TODO:
\begin{enumerate}
\item Variable structure learning automata uitleggen
\item $L_{R-p}, L_{R-\epsilon p} \text{en} L_{R-I}$ uitleggen
\item $\epsilon$-optimality en absolutely expediency uitleggen en melden dat $L_{R-I}$ deze eigenschappen bezit
\item LRUF-tuple uitleggen
\end{enumerate}

\begin{equation}
p_j(k+1)=	\begin{cases}	p_j(k)+a[1-p_j(k)] 	&\mbox{if } j=i \\ 
							(1-a)p_j(k) 		&\mbox{otherwise} 
			\end{cases} 
\label{eq:lruf_reward_update}
\end{equation}

\begin{equation}
g_i=\frac{1}{N_i}\sum\nolimits_{d_{i}^{j}\in \underline{f}_{i}} a(r_{i}^{j})^{-1}
\label{eq:lruf_average_relevance}
\end{equation}

\begin{algorithm}
 \KwData{Query $q_i$, Number of results $n_i$}
 Assume:
 	Let $A_i$ be the learning automaton corresponding to query $q_i$ with action-set $\alpha_i$\\
 	Action $\alpha_i^j \in \alpha_i$ is associated with document $d_i^j \in \underline{d_i}$\\
 	Let $k$ denote the stage number\\
 	Let $G$ be the total relevance score\\
 Initialise: $k\leftarrow 1$, $T_i\leftarrow 0$\\
 \While{$k \le n_i$}{
 	$A_i$ chooses one of its actions (e.g. $a_i^k$) at random\\
 	Document $d_i^j$ corresponding to selected action $\alpha_i^j$ is ranked at $K^{th}$ position of $\underline{R_i}$\\
 	Configuration of $A_i$ is updated by disabling action $\alpha_i^j$\\
 	$k\leftarrow k+1$
 }
 Ranking $\underline{R_i}$ is shown to the user\\
 $N_i\leftarrow 0,\underline{f_i}\leftarrow \emptyset,G\leftarrow 0$\\
 \Repeat{query session is expired}{
 	\For{every document $d_i^j$ visited by user}{
 		$\underline{f_i}\leftarrow \underline{f_i}+{d_i^j}$\\
 		$G\leftarrow G+a*(r_i^j)^{-1}$
 	}
 	$N_i\leftarrow N_i +1$
 }
 $g_i \leftarrow \frac{G}{N_i}$\\
 Configuration of $A_i$ is updated by re-enabling all disabled actions\\
 \If{$g_i\ge T_i$}{
 	Reward the actions corresponding to all visited documents by Equation \ref{eq:lruf_reward_update}\\
 	$T_i\leftarrow g_i$
 }
 \For{$\forall \alpha_i^j \in \underline{\alpha_i}$}{
 	\If{$p_i^j < T_{\epsilon}$}{
 		$d_i^j$ is replaced by another document of the searched results
 	}
 }
 Output $\underline{R_i}$
 \caption{Learning algorithm of LRUF, obtained from \cite{Torkestani2012b}}
 \label{alg:lruf}
\end{algorithm}