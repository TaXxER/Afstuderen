The learning algorithms of the well-performing Learning-to-Rank methods selected in Chapter \ref{chap:cross_comparison} are presented and explained in this Chapter. The Learning-to-Rank methods will be discussed in order of an increasing degree of certainty and decreasing ranking accuracy, as concluded in Chapter \ref{chap:cross_comparison}.

\section{ListNet}
ListNet \cite{Cao2007} is a listwise ranking function whose loss function is not directly related to information retrieval evaluation metrics. ListNet's loss function is defined using a probability distribution on permutations. Probability distributions on permutations have been a research topic within the field of probability theory which has been extensively researched. ListNet uses the Plackett-Luce model \cite{Plackett1975,Luce1959}, which is one of the most well-known permutation probability distributions. The Plackett-Luce model defines the probability of all possible permutations $\pi$, given all document ranking scores $S$, as shown in Equation \ref{eq:plackett_luce}.
\begin{equation}
P(\pi|S) = \prod\limits_{j=1}^{m}\frac{\phi(s_{\pi^{-1}(j)})}{\sum\nolimits_{u=1}^{m}\phi(s_{\pi^{-1}(u)})}
\label{eq:plackett_luce}
\end{equation}
ListNet uses Gradient Descent to optimise a neural network such that its Cross Entropy loss compared to the Plackett-Luce distribution over the training data relevance labels is minimal. Note that some sources, including Liu \cite{Liu2007}, describe ListNet as using \ac{KL divergence} as loss function. \ac{KL divergence} and Cross Entropy are however identical up to an additive constant when comparing distribution $q$ against a fixed reference distribution $p$. This follows from the definition of Cross Entropy Loss, which is shown in Equation \ref{eq:cross_entropy}.
\begin{equation}
H(p,q) = H(p) + D_{KL}(p||q)
\label{eq:cross_entropy}
\end{equation}
where $H(p)$ is the entropy of $p$ and $D_{KL}(p||q)$ is the \ac{KL divergence} of $q$ from $p$.\\

\noindent Equation \ref{eq:gradient_descent} describes the gradient descent step to minimise loss function $L(y^{(i)},z^{(i)}(f_\omega))$ with respect to parameter $\omega$. 
\begin{equation}
\Delta\omega = \frac{\partial L(y^{(i)},z^{(i)}(f_\omega))}{\partial \omega} = - \sum_{\forall g \in \mathscr{G}_k}\limits\frac{\partial P_{z^{(i)}(f_\omega)}(g)}{\partial \omega}\frac{P_{y^{(i)}}(g)}{P_{z^{(i)}(f_\omega)}(g)}
\label{eq:gradient_descent}
\end{equation}

\noindent Algorithm \ref{alg:listnet} shows the pseudo-code of the ListNet training phase.\\
\LinesNumbered
\begin{algorithm}[H]
 \KwData{training data \{$(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),...,(x^{(m)},y^{(m)})$\}}
 \KwIn{number of iterations $T$ and learning rate $\eta$}
 Initialize parameter $\omega$\\
 \For{$t\leftarrow 1$ \KwTo $T$}{
 	\For{$i\leftarrow 1$ \KwTo $m$}{
 		Input $x^{(i)}$ of query $q^{(i)}$ to Neural Network and, for the current value of $\omega$, compute score list $z^{(i)}(f_\omega)$.\\
 		Compute gradient $\Delta\omega$ using Eq. (\ref{eq:gradient_descent}).\\
 		Update $\omega = \omega - \eta \times \Delta\omega$.
 	}
 }
 Output Neural Network model $\omega$.
 \caption{Learning algorithm of ListNet, obtained from \cite{Cao2007}}
 \label{alg:listnet}
\end{algorithm}

\section{SmoothRank}
SmoothRank \cite{Chapelle2010} is a listwise ranking method that, in contrast to ListNet, directly optimises an information retrieval evaluation measure. SmoothRank enables direct optimisation of the non-convex and discontinuous evaluation measure by smoothing, that is, approximating the rank position. In this section, I apply the SmoothRank approximation method to the \ac{nDCG} evaluation measure for illustration, but the same procedure can be applied to \ac{MAP} or any other information retrieval measure. The smoothing function used in SmoothRank is based on the softmax activation function \cite{Bridle1990}, which is often used in neural networks. The softmax function is formally defined as shown in Equation \ref{eq:softmax_algorithm}.
\begin{equation}
p_i = \frac{e^{f_i/\sigma}}{\sum\nolimits_{j=1}^{m}e^{f_j/\sigma}}
\label{eq:softmax_algorithm}
\end{equation}

where $\sigma$ is the smoothing parameter. Chapelle et al. \cite{Chapelle2010} apply this softmax function to the \ac{nDCG} formula and hereby introduce a soft version of indicator variable $h_{i,j}$ through the computation shown in Equation \ref{eq:soft_ndcg}.

\begin{equation}
h_{i,j} = e^{-\frac{(f(x_i))-(f(x_{d(j)}))^2}{\sigma}}\Big/\sum\nolimits_{k=1}^{m}e^{-\frac{(f(x_k))-f(x_{d(j)})^2}{\sigma}}
\label{eq:soft_ndcg}
\end{equation}

It can be shown that the derivative of the smoothed \ac{nDCG} version shown in Equation \ref{eq:soft_ndcg} and the smoothed versions of other \ac{IR} metrics can be calculated in $\mathcal{O}(m^2)$, which enable fast gradient descent optimisation. The optimisation step in SmoothRank uses the nonlinear conjugate gradient method  with Polak-Ribiere update \cite{Shewchuk1994}, which is a type of gradient descent method. This optimisation method is prone to local optima, which is alleviated by adding a pre-calculated, better than naive, starting point and by adding a regularisation term.\\

The starting point in SmoothRank is set to either the solution of a simple linear regression, or alternatively to the solution of Rank\acs{SVM}. Since this starting point is expected to already be a good solution, a regulariser term is added to the SmoothRank objective function to prevent the solution from deviating to much from the starting point. The regularised smooth objective function is formulated as $\lambda||w-w_0||_2$ where $\lambda$ is a hyper-parameter tuned on the validation set, $w_0$ is the starting point solution, and $||x||_2$ is the $\ell2$-norm regularisation function defined as $||x||_2 = \sqrt{\sum\nolimits_ix_i^2}$.\\

The choice of the smoothing parameter $\sigma$ in Equation \ref{eq:soft_ndcg} is important, because a too small value makes the function more non-smooth and therefore harder to optimise, while a too large value results in a optimisation function with optima that substantially differ from the true optimal rankings. To deal with the problem of choosing the smoothing parameter, SmoothRank uses an annealing method where the optimisation procedure starts with a large $\sigma$ and iteratively reduces it by dividing it by two at each iteration. Algorithm \ref{alg:smoothrank} shows the algorithm that summarises all steps of the SmoothRank method.\\
\LinesNumbered
\begin{algorithm}[H]
 Find an initial solution $w_0$ (by regression or Rank\acs{SVM}).\\
 Set $w = w_0$ and $\sigma$ to a large value.\\
 \While{Stopping condition not satisfied}{
 	 Starting from $w$, minimize by non-linear conjugate gradient descent:
 	 $\lambda||w-w_0||_2 - \sum\nolimits_{q}A_q(w,\sigma)$\\
 	 $\sigma = \sigma/2$
 }
 \caption{Learning algorithm of SmoothRank, obtained from \cite{Chapelle2010}}
 \label{alg:smoothrank}
\end{algorithm}

\section{FenchelRank}
FenchelRank \cite{Lai2013} is a ranking method that addresses the sparse Learning-to-Rank problem, which is the problem of learning a ranking function with only a few non-zero coefficients with respect to the input features. FenchelRank is based on the theory of Fenchel Duality \cite{Rifkin2007} and uses a generic algorithm framework proposed by Shalev-Shwartz and Singer \cite{Shalev-Shwartz2010}. FenchelRank optimises the objective function shown in Equation \ref{eq:pairwise_l1_loss} which is equivalent to the standard pairwise loss function with $\ell_1$-norm regularisation. The $\ell_1$-norm regularisation function in this equation is represented by $||x||_1$ and is defined as $||x||_1=\sum\nolimits_i|x_i|$.
\begin{equation}
\min_w G(w) = \min_w I_{||w||_{1} \le 1}(w) + \frac{r^2}{p} \sum\limits_{i=1}^{p}\max(0,\frac{1}{r}-(Kw)_i)^2
\label{eq:pairwise_l1_loss}
\end{equation}

\noindent where $I_{C}(w)$ is a function that is 0 if condition $C$ is satisfied, and $\infty$ otherwise. $m$ is the dimension of the data, $p$ is the number of comparable object pairs, $K$ is a matrix in $\mathbb{R}^{p \times m}$ that contains pairwise information. The objective function in Equation \ref{eq:pairwise_l1_loss} is not differentiable everywhere because of its $\ell_1$-norm regularisation term. Fenchel's duality theorem \cite{Rifkin2007}, defined in Equation \ref{eq:fenchel_duality_theorem}, provides a way to approximate the optimal value of this non-differentiable optimisation problem by instead solving the Fenchel dual of the optimisation problem.
\begin{equation}
\min_x(f(x)-g(x)) = \max_p(f_*(p)-f^*(p))
\label{eq:fenchel_duality_theorem}
\end{equation}
\noindent where $f^*$ is the convex, and $f_*$ the concave conjugate of $f$.\\

To ease applying Fenchel's duality theorem (Equation 7) to the $\ell_1$-regularised pairwise loss function (Equation \ref{eq:pairwise_l1_loss}), Lai et al. define $D(w)$ as $D(w) = -G(w)$. Equation \ref{eq:fenchel_dual} shown the resulting Fenchel dual of the $\ell_1$-regularised pairwise loss function, which is the loss function that is used in FenchelRank.
\begin{equation}
\max_w D(w) = \max_w I_{||w||_{1} \le 1}(w) - \frac{r^2}{p} \sum\limits_{i=1}^{p}\max(0,\frac{1}{r}-(Kw)_i)^2
\label{eq:fenchel_dual}
\end{equation}

Algorithm \ref{alg:fenchelrank} shows the FenchelRank training algorithm to optimise the Fenchel dual of the pairwise loss function. The $||x||_\infty$ term in this algorithm represents an $\ell_\infty$-norm regularisation term and is defined as $||x||_\infty=\max_{i}|x_i|$.\\

\LinesNumbered
\begin{algorithm}[H]
 \KwData{pairwise training data matrix $K$}
 \KwIn{desired accuracy $\epsilon$, maximum number of iterations $T$ and the radius $r$ of the $\ell1$ ball.}
 Initialize: $w_1$ = $0_m$\\
 \For{$t\leftarrow 1$ \KwTo $T$}{
 	//check if the early stopping criterion is satisfied\\
 	\If{$||g_{t}||_{\infty} + \langle d_{t}, -Kw_{t} \rangle \le \epsilon$}{
 	//here $d_{t} = \nabla f^{*}(-Kw_{t}) = \frac{\partial f^{*}(-Kw)}{\partial(Kw)}|w=w_{t}$, \\
 	// $\langle x,y \rangle$ represents the inner products of vectors $x$ and $y$, and\\
 	// $g_{t} = d_{t}^{T}K$\\
 	return $w_{t}$ as ranking predictor $w$
 	}
 //greedily choose a feature to update\\
 Choose $j_{t} = \argmax_{j}|(g_t)_j)|$\\
 //compute an appropriate step size\\
 Let $\mu_t = \argmax_{0 \le \mu_{t} \le 1} D((1-\mu_{t})w_{t} + \mu_{t}\sign((g_{t})_{j_{t}})e^{j_{t}})$\\
 //update the model with the chosen feature and step size\\
 Update $w_{t+1}=(1-\mu_{t})w_{t} + \mu_{t}\sign((g_{t})j_{t})e^{j_{t}}$
 }
 return $w_{T}$ as ranking predictor for $w$
 \caption{Learning algorithm of FenchelRank, obtained from \cite{Lai2013}}
 \label{alg:fenchelrank}
\end{algorithm}

\section{FSMRank}
Lai et al. \cite{Lai2013c} observed that existing feature selection methods in Learning-to-Rank all follow a two-stage paradigm consisting of a first stage of selecting a subset of features from the original features, followed by a second stage where a ranking model learnt based on the selected features. Lai et al. \cite{Lai2013c} state it as a limitation of this paradigm that the selected features in the first step are not necessarily the optimal features for the second stage where the ranking model is build. FSMRank \cite{Lai2013c} addresses this limitation by formulating a joint convex optimisation function that minimises ranking errors while simultaneously selecting a subset of features.\\

FSMRank uses an extended version of gradient descent optimisation, proposed by Yuri Nesterov, that enables faster convergence for convex problems \cite{Nesterov2004}. Nesterov's accelerated gradient descent can guarantee an $\epsilon$-accurate solution in at most $T$ iterations where $\epsilon=\mathcal{O}(1/T^2)$.\\

Let $S = {(q_k, \hat{X}^{(q_k), Y^{(q_k)}})}_{k=1}^n$ be a training set with queries $q_k$, corresponding retrieval objects $\hat{X}^{(q_k)}$, and corresponding relevance labels $Y^{(q_k)}$. Let $||x||_1$ be the $l_1$-norm regularisation function that is defined as $||x||_1 = \sum\nolimits_i |x_i|$. Let $\oslash$ be the element-wise division operator. $\hat{A}$ is a $d \times d$ similarity matrix that contains similarity scores between features. The convex joint optimisation function in FSMRank is defined as shown in Equation \ref{eq:fsmrank_optimisation_function}.
\begin{equation}
\min_{\hat{w}} \frac{\lambda_1}{2} \hat{w}^T \hat{A}\hat{w} + \lambda_2 ||\hat{w}\oslash\hat{s}||_1 + f(\hat{w}, (\hat{X}^{(q_k)}, Y^{(q_k)})_{k=1}^n)
\label{eq:fsmrank_optimisation_function}
\end{equation}
\noindent The first term in Equation \ref{eq:fsmrank_optimisation_function} applies a penalty on redundancy of large weighted features by using the $\hat{A}$ matrix. The well-known Pearson correlation coefficient is used to calculate similarity matrix $\hat{A}$, based on the values for the features in the training data. $\lambda_1$ is a hyper-parameter of the model and can be used to set the weight of the similarity penalty term.\\

The second term of Equation \ref{eq:fsmrank_optimisation_function} contains a $\ell_1$-norm regularisation term to select the effective features from the feature set. $\lambda_2$ is a hyper-parameter of the model and can be used to set the weight of the regularisation term.\\

The third and last term of the equation represents the loss function in terms of ranking errors. Loss function $f$ can in theory be any convex loss function, but Lai et al. \cite{Lai2013c} used a squared hinge loss function for their evaluation measurements. 

Algorithm \ref{alg:fsmrank} shows the steps of the FSMRank training algorithm. As stated, FSMRank uses an  extended version of Nesterov's accelerated gradient method. This optimisation method can handle optimisation problems in the form of $\min_w \l(w) + r(w)$ where $l(w)$ is a convex function with Lipschitz gradient and $r(w)$ is convex, but non-smooth. The optimization function of Equation \ref{eq:fsmrank_optimisation_function} is reformulated to match this form as presented in Equation \ref{eq:fsmrank_optimisation_reformulated}.
\begin{equation}
\argmin_{w_t:w_t\in \mathbb{R}} Q(w_t,z_t) = \lambda_2 \sum\nolimits_{i=1}^{2d}\frac{w_i}{s_i}+\langle l^{'}(z_t),w_t-z_t \rangle+\frac{L}{2}||w_t-z_t||^2
\label{eq:fsmrank_optimisation_reformulated}
\end{equation}
\noindent where $Q(w_t,z_t)$ is a combination of the non-smooth part $r(w_t)$ and a quadratic approximation of the smooth part $l(w_t)$. $\lambda_1$, $\lambda_2$ and Lipschitz constant $L_0$ are input parameters of the algorithm and can be optimised through cross-validation or on a validation set.\\

\LinesNumbered
\begin{algorithm}[H]
 \KwData{training set $S = {(q_k,\hat{X}^{(q_k)},Y^(q_k)}_{k=1}^{n}$}
 \KwIn{$\lambda_1, \lambda_2, T \text{ and } L_0$}
 Initialize: $w_0=z_1=0, \alpha_1=1, \gamma=2, L=L_0/\gamma^{10}$\\
 \For{$t\leftarrow 1$ \KwTo $T$}{
 	Let $g = l^{'}(z_t)$\\
 	\While{true}{
 		//projection step\\
 		$w_t= \argmin_{w_t:w_t\in \mathbb{R}_{+}^{2d}} Q(w_t,z_t)$\\
 		\If{$l(w_t) \le l(z_t) + \langle l^{'}(z_t),w_t-z_t \rangle +\frac{L}{2}||w_t-z_t||^2$}{
 			break\\
 		}
 		$L = \lambda L$ 		
 	}
 	\If{$\frac{|F(w_t)-F(w_{t-1})|}{|F(w_{t-1})|} \le \epsilon_s$}{
 		//early stopping criterion\\
 		break
 	}
 	$\alpha_{t+1}=\frac{1+\sqrt{1+4\alpha_t^2}}{2}$\\
 	$z_{t+1} = w_t + \frac{\alpha_t-1}{\alpha_t+1}(w_t-w_{t-1})$ 
 }
 
 \caption{Learning algorithm of FSMRank, obtained from \cite{Lai2013c}}
 \label{alg:fsmrank}
\end{algorithm}

\section{LRUF}
LRUF \cite{Torkestani2012b} is Learning-to-Rank method based on the theory of learning automata. Learning automata are adaptive decision-making units that learn the optimal set of actions through repeated interactions with its environment. Variable structure learning automata can be defined as a triple $<\beta,\alpha,L>$, with $\beta$ being the set of inputs, $\alpha$ the set of actions. $L$, the learning algorithm ,is a recurrence relation that modifies the action probability vector of the actions in $\alpha$.\\

Three well-known learning algorithms $L$ used in variable structure learning automata are \emph{linear reward-penalty} ($L_{R-P}$), \emph{linear reward-$\epsilon$-penalty} ($L_{R-\epsilon P}$) and \emph{linear reward-inaction} ($L_{R-I}$). LRUF uses the $L_{R-I}$ learning algorithm, which updates the action probability vector following Equation \ref{eq:lruf_reward_update} when the selected action $a_i(k)$ is rewarded by the environment.
\begin{equation}
p_j(k+1)=	\begin{cases}	p_j(k)+a[1-p_j(k)] 	&\mbox{if } j=i \\ 
							(1-a)p_j(k) 		&\mbox{otherwise} 
			\end{cases} 
\label{eq:lruf_reward_update}
\end{equation}
\noindent where $i$ and $j$ are indices of action in $\alpha$ and $p(k)$ is the probability vector over the action set at rank $k$. $a$ is the learning rate of the model.\\

A variable action-set learning automaton is an automaton in which the number of available actions change over time. It has been shown that a combination of a variable action-set learning automaton with the $L_{R-I}$ learning algorithm is absolutely expedient and $\epsilon$-optimal \cite{Thathachar1987}, which means that it is guaranteed to approximate the optimal solution to some value $\epsilon$ and each update step is guaranteed not to decrease performance of the model. A variable-action set learning automata has a finite set of $r$ actions $\alpha={\alpha_1,...,\alpha_r}$. $A$ is defined as the power set of $\alpha$, $A={A_1,..,A_m}$ with $m = 2^{r}-1$. $\psi(k)$ is a probability distribution over $A$ such that $\psi(k)=p(A(k)=A_i|A_i\in A, 1\le i \le 2^r-1)$. $\hat{p}_i(k)$ is the probability of choosing action $\alpha_i$ given that action subset $A(k)$ has already been selected and $\alpha_i \in A(k)$.\\

LRUF uses an optimisation problem that can be illustrated as a quintuple $<q_i,A_i,\underline{d_i},\underline{R_i},\underline{f_i}>$, where $q_i$ is a submitted query, $A_i$ is a variable action-set learning automaton, $\underline{d_i}$ is a set of documents associated with $q_i$, and $\underline{R_i}={r_i^j|\forall d_i^j \in \underline{d_i}}$ is a ranking function that assigns rank $r_i^j$ to each document. $\underline{f_i}$ is a feedback set used for the update step described in Equation \ref{eq:lruf_reward_update}. For each rank $k$ the learning automaton $A_i$ chooses one of its actions following its probability vector, jointly forming $\underline{R_i}$. LRUF translates the feedback in $\underline{f_i}$ to an understandable value to use it to converge the action probability vector in optimal configuration. Therefore LRUF defines a $g_i: \underline{f_i} \rightarrow \mathbb{R}^{+}$ to be a mapping from the feedback set into a positive real number. The LRUF mapping function $g_i$ computes the average relevance score of ranking $R_i$ based on $f_i$ and is defined as shown in Equation \ref{eq:lruf_average_relevance}.
\begin{equation}
g_i=\frac{1}{N_i}\sum\nolimits_{d_{i}^{j}\in \underline{f_i}} a(r_{i}^{j})^{-1}
\label{eq:lruf_average_relevance}
\end{equation}
\noindent in which $N_i$ refers to the size of feedback set $\underline{f_i}$, $r_i^j$ is the rank of document $d_i^j$ in $R_i$ and, again, $a$ denotes the learning rate.\\

Initially, before any feedback, the action probability factors are initialised with equal probability.

In case the document set of the search engine changes, i.e. new documents are indexed or old documents are removed, LRUF has an Increase Action-Set Size (IAS) and a Reduce Action-Set Size (RAS) procedure defined to adapt to the new document set without needing a complete retraining of the model. Because this thesis focusses on model accuracy and scalability the IAS and RAS procedure of LRUF will not be explained in further detail.

Torkestani \cite{Torkestani2012b} does not provide pseudo code specifically for the training phase of the LRUF algorithm. Instead an algorithm including both ranking and automaton update is presented, and included in Algorithm \ref{alg:lruf}. Initial training of the algorithm can be performed by using the training data relevance labels in the feedback set of the algorithm.

\begin{algorithm}
 \KwData{Query $q_i$, Number of results $n_i$}
 Assume:
 	Let $A_i$ be the learning automaton corresponding to query $q_i$ with action-set $\alpha_i$\\
 	Action $\alpha_i^j \in \alpha_i$ is associated with document $d_i^j \in \underline{d_i}$\\
 	Let $k$ denote the stage number\\
 	Let $G$ be the total relevance score\\
 Initialise: $k\leftarrow 1$, $T_i\leftarrow 0$\\
 \While{$k \le n_i$}{
 	$A_i$ chooses one of its actions (e.g. $a_i^k$) at random\\
 	Document $d_i^j$ corresponding to selected action $\alpha_i^j$ is ranked at $K^{th}$ position of $\underline{R_i}$\\
 	Configuration of $A_i$ is updated by disabling action $\alpha_i^j$\\
 	$k\leftarrow k+1$
 }
 Ranking $\underline{R_i}$ is shown to the user\\
 $N_i\leftarrow 0,\underline{f_i}\leftarrow \emptyset,G\leftarrow 0$\\
 \Repeat{query session is expired}{
 	\For{every document $d_i^j$ visited by user}{
 		$\underline{f_i}\leftarrow \underline{f_i}+{d_i^j}$\\
 		$G\leftarrow G+a*(r_i^j)^{-1}$
 	}
 	$N_i\leftarrow N_i +1$
 }
 $g_i \leftarrow \frac{G}{N_i}$\\
 Configuration of $A_i$ is updated by re-enabling all disabled actions\\
 \If{$g_i\ge T_i$}{
 	Reward the actions corresponding to all visited documents by Equation \ref{eq:lruf_reward_update}\\
 	$T_i\leftarrow g_i$
 }
 \For{$\forall \alpha_i^j \in \underline{\alpha_i}$}{
 	\If{$p_i^j < T_{\epsilon}$}{
 		$d_i^j$ is replaced by another document of the searched results
 	}
 }
 Output $\underline{R_i}$
 \caption{LRUF algorithm, obtained from \cite{Torkestani2012b}}
 \label{alg:lruf}
\end{algorithm}
