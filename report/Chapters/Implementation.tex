This chapter describes implementation details of the Learning-to-Rank algorithm parallel implementations in HDInsight that are either Hadoop, Microsoft Azure, or HDInsight and are not part of the algorithm specification itself. The first section of this chapter will briefly discuss the HDInsight platform, the Hadoop ecosystem components offered by HDInsight and, in this regard, the Hadoop components used for implementation of the algorithms described in Chapter \ref{chap:ltr_methods}.

\section{Architecture}
Ranking models almost without exception consist of a sequence of operations on the input data, that are in most cases repeatedly executed in a series of iterations. Apache Pig will be used to implement the sequence of operation on the input data, because the higher level of abstraction that Apache Pig offers makes it easier to focus solely on the data operations as compared to native Hadoop MapReduce.\\

% TODO: Stuk over werkzaamheden framework uitbreiden: uitlezen resultaten (evaluatieresultaten + doorgeven tussenresultaten (die tweede wellicht later in het document pas introduceren))
In a cross-validation evaluation experiment of a ranking model, the handling of which folds are loaded for training, evaluation and testing is a task that need to be taken care of regardless of the ranking model. Given that most ranking models are of an iterative nature, the task of iteration handling is also a task that need to be done for (most) models. Since these tasks need to be considered for (almost) all ranking models it seems natural to develop a framework that takes care of these tasks. The aim of this framework is to let implementations of ranking models focus solely on implementing the sequential steps of one iteration of the algorithm, while the framework arranges that 1) these sequential steps are performed iteratively and 2) these sequential steps are performed on the multiple training folds of data. This framework that handles folding and iterations will be implemented in Java and will work such that fold- and iteration dependent parts of the Pig code will be generated dynamically by the Java framework after which the Pig job will be sent to the cluster.\\

\subsection{Microsoft Azure HDInsight}
Microsoft offers a scalable and on-demand Hadoop service with HDInsight, that enables Hadoop services for those not able to make the required investments for their own cluster. The latest HDInsight of HDInsight available at the time of writing, HDInsight 3.1, runs the Hortonworks distribution of Hadoop version 2.4. HDInsight 3.1 offers multiple ways of submitting Hadoop jobs to a HDInsight cluster, described in Table \ref{tbl:hdinsight_endpoints}.\\

\begin{table}
\centering
\begin{tabular}{p{4cm}p{2.8cm}p{5.5cm}}\toprule
Job submission method & Type & Description \\
\midrule
Powershell & Powershell scripts & The Azure module for Windows PowerShell enables direct submission of Hadoop jobs through PowerShell cmdlets.\\
C\# API & C\# API & A wrapper API is offered to submit Hadoop MapReduce jobs directly from C\# code.\\
HiveServer/HiveServer2 & REST endpoint & HiveServer and its successor HiveServer 2 are, as its name suggests, REST endpoints that allow remote submission of Hive queries.\\
Oozie & REST endpoint & Apache Oozie is a workflow scheduler to manage Apache Hadoop jobs. Oozie enables users to specify Directed Acyclical Graphs of action, where each action is specified in either MapReduce or Pig.\\
WebHCat/Templeton & REST endpoint & WebHCat, formerly known as Templeton, is a REST API for HCatalog, a table and storage management layer for Hadoop. WebHCat allows users to use either Apache Pig, Apache MapReduce or Apache Hive for data processing.\\	
\bottomrule
\end{tabular}
\caption{HDInsight REST endpoints for job submission}
\label{tbl:hdinsight_endpoints}
\end{table}

Oozie and WebHCat/Templeton are the two methods for job submission in Table \ref{tbl:hdinsight_endpoints} that both 1) support Apache Pig jobs, and 2) Can be used from within Java code. Below the necessary procedures for job submission from Oozie as well as from WebHCat will be sketched.\\

\begin{table}
\centering
\begin{tabular}{p{5cm}p{5cm}}\toprule
Job submission procedure for Apache Oozie & Job submission procedure for WebHCat/Templeton \\
\midrule
1. Let the framework build the Pig job dynamically. & 1. Let the framework build the Pig job dynamically.\\
2. Encapsulate the Pig job in an Oozie workflow. & 2. Submit the Pig job through the WebHCat/Templeton REST API.\\
3. Upload the Oozie workflow to HDFS storage. & \\
4. Execute the Oozie workflow through the Oozie REST API. & \\
\bottomrule
\end{tabular}
\caption{Comparison of Oozie and WebHCat job submission procedures}
\label{tbl:oozie_templeton}
\end{table}

Table \ref{tbl:oozie_templeton} shows how Oozie job submission is more complex then for the case of dynamically generated jobs than WebHCat/Templeton. Oozie is more fitting for static Hadoop jobs that require a workflow consisting of a sequence of Pig and MapReduce jobs mixed together, but is a lesser fit for our situation. Templeton will be used for submission of the Pig jobs, as the HDInsight version that was available at the start of the implementation did not yet support WebHCat.

\subsection{ListNet}
The following sections describe the Pig jobs that form the three independent parts of the ListNet ranking model: 1) Preprocessing, 2) Training (this includes evaluation over the validation set) and 3) Testing (evaluation over the test set). 
\subsubsection{Preprocessing: feature scaling}
The preprocessing phase consists of two separate Pig jobs. The first Pig job determines the minimum and the maximum values per feature in the training set. The second Pig job rescales each feature of the train, validation and test datasets using the following formula for rescaling:
\begin{equation}
x^{'} = \frac{x-min(x)}{max(x)-min(x)}
\end{equation}
This rescaling procedure set the range of each feature to $[0,1]$. 


The first Pig job:
\begin{lstlisting}
REGISTER wasb:///user/hdp/lib/*.jar;
TRAIN = LOAD '[path prefix]/input/[dataset name]/Fold[fold number]/train.txt' USING PigStorage(' ');
TRAIN_STD = FOREACH TRAIN GENERATE flatten(udf.util.ToStandardForm(*));
TRAIN_STD_BY_QUERY = GROUP TRAIN_STD BY $1 PARALLEL [available reducers];
MIN_MAX = FOREACH TRAIN_STD_BY_QUERY GENERATE flatten(udf.util.GetMinMax(*));
MIN_MAX_GRPD = GROUP MIN_MAX ALL;
MIN_MAX_FIN = FOREACH MIN_MAX_GRPD GENERATE flatten(udf.util.CombineMinMax(*));
STORE MIN_MAX_FIN INTO 'minmax[fold number]';
\end{lstlisting}
The second Pig job:
\begin{lstlisting}
REGISTER wasb:///user/hdp/lib/*.jar;
TRAIN = LOAD '[path prefix]/input/[dataset name]/Fold[fold number]/train.txt' USING PigStorage(' ');
VALIDATE = LOAD '[path prefix]/input/[dataset name]/Fold[fold number]/vali.txt' USING PigStorage(' ');
TEST = LOAD '[path prefix]/input/[dataset name]/Fold[fold number]/test.txt' USING PigStorage(' ');
TRAIN_STD = FOREACH TRAIN GENERATE flatten(udf.util.ToStandardForm(*));
VALIDATE_STD = FOREACH VALIDATE GENERATE flatten(udf.util.ToStandardForm(*));
TEST_STD = FOREACH TEST GENERATE flatten(udf.util.ToStandardForm(*));
DEFINE ScaleFeatures udf.util.ScaleFeatures('"+LtrUtils.toParamString(minmaxList)+"');
TRAIN_SCA = FOREACH TRAIN_STD GENERATE flatten(ScaleFeatures(*));
VALIDATE_SCA = FOREACH VALIDATE_STD GENERATE flatten(ScaleFeatures(*));
TEST_SCA = FOREACH TEST_STD GENERATE flatten(ScaleFeatures(*));
STORE TRAIN_SCA INTO 'train_sca[fold number]' USING BinStorage();
STORE VALIDATE_SCA INTO 'validate_sca[fold number]' USING BinStorage();
STORE TEST_SCA INTO 'test_sca[fold number]' USING BinStorage();
\end{lstlisting}
\subsubsection{}