\label{chap:implementation}
The first section of this chapter will briefly discuss the HDInsight platform, the Hadoop ecosystem components offered by HDInsight and, in this regard, the Hadoop components used for implementation of the algorithms described in Chapter \ref{chap:ltr_methods}. The second section of this chapter describes a Java framework that handles the joint components needed for MapReduce Learning to Rank computation that are independent of the Learning to Rank model. The subsequent sections describe the implementation details of specific Learning to Rank models.

\section{Architecture}
Ranking algorithms consist of a sequence of operations on the input data which are often, but not always, of iterative nature. Apache Pig \cite{Olston2008} will be used to implement the sequence of operations on the input data. Pig Latin, the data processing language that runs on Apache Pig, was designed based on the observation that the traditional MapReduce paradigm is too low-level and rigid, and holds the middle between the declarative style of SQL and the procedural style of MapReduce. The Apache Pig system translates Pig Latin into MapReduce plans that are executed over Hadoop. The choice to implement the Learning to Rank algorithms in Pig Latin allows for more focus on the data operations and less focus on low-level implementation details, as compared to native Hadoop MapReduce. Furthermore, it allows us to rely on Apache Pig to create efficient MapReduce plans out of the Pig Latin code and therefore lowers the implementation-dependent factor of the experiments.\\

\subsection{The HDInsight Platform
Azure HDInsight supports both the traditional \ac{HDFS} as described by Shvacko et al. \cite{Shvachko2010}, as well as Microsoft's own storage solution \ac{WASB}. Blob storage decouples the storage from the HDInsight Hadoop cluster; it enables safe deletion of a HDInsight cluster without losing data, as data is not solely stored on the cluster itself, but also on a separate storage that is not cluster-dependent. Azure \ac{WASB} storage allows the user to select one of Microsoft's data centres for storage. \ac{WASB} storage in the West Europe region (located in Amsterdam) will be used for storage, as this data centre is located close to where the experiments are executed.
}
Microsoft offers a scalable and on-demand Hadoop service with HDInsight, which enables Hadoop services for those not able to make the required investments for their own cluster. The latest HDInsight version available at the time of writing, HDInsight 3.1, runs the Hortonworks distribution of Hadoop version 2.4. While early versions of Hadoop were merely an open source implementation of MapReduce, newer versions since Hadoop 2.0 offer support for variety of programming models with the introduction of Hadoop YARN \cite{Vavilapalli2013}. Newly supported programming models since Hadoop 2.0 include Dryad \cite{Isard2007}, Giraph \cite{Avery2011}, MPI, REEF \cite{Chun2013}, Spark \cite{Zaharia2010}, and Storm \cite{Aniello2013}. Even though these programming models are now supported by Hadoop and even though some of these programming models have recently increased in popularity, they still lack the critical mass as the data processing framework of choice as Lin argued back in 2012 \cite{Lin2013}. Therefore, even though some of these programming models might be a better fit for iterative algorithms, we use Hadoop MapReduce of the programming model to implement the Learning to Rank algorithms.\\

HDInsight 3.1 offers multiple ways of submitting Hadoop jobs to a HDInsight cluster, described in Table \ref{tbl:hdinsight_endpoints}. Oozie and WebHCat/Templeton are the two methods for job submission in Table \ref{tbl:hdinsight_endpoints} that both 1) support Apache Pig jobs, and 2) Can be used from within Java code. Table \ref{tbl:oozie_templeton} shows the necessary procedures for job submission from Oozie as well as from WebHCat will be sketched. Table \ref{tbl:oozie_templeton} shows Oozie job submission to be more complex for the case of dynamically generated jobs than WebHCat/Templeton. Oozie is more fitting for static Hadoop jobs that require a workflow consisting of a sequence of Pig and MapReduce jobs mixed together, but is a lesser fit for our situation. Templeton will be used for submission of the Pig jobs, as the HDInsight version that was available at the start of the implementation did not yet support WebHCat.\\

\begin{table}
\centering
\begin{tabular}{p{4.2cm}p{2.6cm}p{5.5cm}}\toprule
Job submission method & Type & Description \\
\midrule
Powershell & Powershell scripts & The Azure module for Windows PowerShell enables direct submission of Hadoop jobs through PowerShell cmdlets.\\
C\# API & C\# API & A wrapper API is offered to submit Hadoop MapReduce jobs directly from C\# code.\\
HiveServer/HiveServer2 & REST endpoint & Apache Hive \cite{Thusoo2009} is an open-source data warehousing solution on top of Hadoop, that supports processing of a SQL-like declarative language called HiveQL. HiveServer and its successor HiveServer 2 are REST endpoints that allow remote submission of HiveQL queries.\\
Oozie & REST endpoint & Apache Oozie \cite{Islam2012} is a workflow scheduler to manage Apache Hadoop jobs. Oozie enables users to specify Directed Acyclical Graphs of action, where each action is specified in either MapReduce or Pig.\\
WebHCat/Templeton & REST endpoint & WebHCat, formerly known as Templeton, is a REST API for HCatalog, a table and storage management layer for Hadoop. WebHCat allows users to use either Apache Pig, Apache MapReduce or Apache Hive for data processing.\\	
\bottomrule
\end{tabular}
\caption{HDInsight REST endpoints for job submission}
\label{tbl:hdinsight_endpoints}
\end{table}

\begin{table}
\centering
\begin{tabular}{p{5cm}p{5cm}}\toprule
Job submission procedure for Apache Oozie & Job submission procedure for WebHCat/Templeton \\
\midrule
1. Let the framework build the Pig job dynamically. & 1. Let the framework build the Pig job dynamically.\\
2. Encapsulate the Pig job in an Oozie workflow. & 2. Submit the Pig job through the WebHCat/Templeton REST API.\\
3. Upload the Oozie workflow to HDFS storage. & \\
4. Execute the Oozie workflow through the Oozie REST API. & \\
\bottomrule
\end{tabular}
\caption{Comparison of Oozie and WebHCat job submission procedures}
\label{tbl:oozie_templeton}
\end{table}

\subsection{Handling repetitive tasks}
Fold handling  in a cross-validation experiment is the process of loading the correct data folds for training, validation and testing in the multiple rounds that the cross-validation experiment consists of. Fold handling is a task that needs to be taken care of regardless of the ranking model that is being evaluated. Given that most ranking models are of an iterative nature, the task of iteration handling is also a task that need to be done for most models. Iteration handling and fold handling are procedures in which the same steps are repeated for each iteration or cross-validation round respectively. Pig Latin, in contrast to more procedural languages that generate MapReduce job like Sawzall \cite{Pike2005}, has no support for loops which are needed to perform iteration handling and fold handling. Since iteration handling and fold handling are tasks that need to be addressed for each ranking model and that cannot be solved with Pig, we create a framework that takes care of both iteration and fold handling and generates the Pig Latin code for the current iteration and fold.\\

Handling communication between Pig jobs within a single iteration of a Learning to Rank algorithm is another problem that needs to be addressed. A Pig Job writes its result to \ac{WASB} storage, while this result is in most cases needed by a subsequent Pig job as parameter. The framework enables reading the result of a Pig job from \ac{WASB} storage which can then be used as parameter within a subsequent Pig job. An example of this is a Pig implementation of gradient descent, where a first Pig job might calculate gradients and writes them to storage, after which the framework assists in reading the gradients from storage and allows them to be used as input parameters of a second Pig job that calculates new feature weights based on its gradient parameter and a predetermined step size.\\

The aim of this framework is to let implementations of ranking models focus solely on implementing the sequential steps of one iteration of the algorithm, while the framework handles that 1) these sequential steps are performed iteratively, 2) these sequential steps are performed on the multiple training folds of data and 3) data can be passed from one Pig job within the algorithm to another Pig job. This framework that handles folding and iterations will be implemented in Java and will work such that fold- and iteration dependent parts of the Pig code will be generated dynamically by the Java framework after which the Pig job will be sent to the cluster.\\

\subsection{Framework description}
TODO

\section{ListNet}
The following sections describe the Pig jobs that form the three independent parts of the ListNet ranking model: 1) Preprocessing, 2) Training (this includes evaluation over the validation set) and 3) Testing (evaluation over the test set). 
\subsection{Preprocessing}
The preprocessing phase consists of two separate Pig jobs. The first Pig job determines the minimum and the maximum values per feature in the training set. The second Pig job rescales each feature of the train, validation and test datasets using the following formula for rescaling:
\begin{equation}
x^{'} = \frac{x-min(x)}{max(x)-min(x)}
\end{equation}
This rescaling procedure sets the values of all features to be within range $[0,1]$.\\

The first Pig job:\\
\begin{minipage}{\linewidth}
\begin{lstlisting}
REGISTER [path prefix]/lib/*.jar;
TRAIN = LOAD '[path prefix]/input/[dataset name]/Fold[fold number]/train.txt' USING PigStorage(' ');
TRAIN_STD = FOREACH TRAIN GENERATE flatten(udf.util.ToStandardForm(*));
TRAIN_STD_BY_QUERY = GROUP TRAIN_STD BY $1 PARALLEL [available reducers];
MIN_MAX = FOREACH TRAIN_STD_BY_QUERY GENERATE flatten(udf.util.GetMinMax(*));
MIN_MAX_GRPD = GROUP MIN_MAX ALL;
MIN_MAX_FIN = FOREACH MIN_MAX_GRPD GENERATE flatten(udf.util.CombineMinMax(*));
STORE MIN_MAX_FIN INTO 'minmax[fold number]';
\end{lstlisting}
\end{minipage}\\

The minimum and maximum values per feature stored in MIN\_MAX\_FIN can now be read from \ac{HDFS} and used within the Java framework by using the Windows Azure Storage library. The second Pig job uses these minimum and maximum values by parametrising the \ac{UDF} that performs the rescaling transformation, udf.util.ScaleFeatures(), with the minimum and maximum feature values.\\

\begin{table}
\centering
\begin{tabular}{p{6cm}p{7cm}}\toprule
UDF & Description \\
\midrule
udf.util.ToStandardForm() & Transforms the data set into the standard form of relevance label in first column followed by feature values. Strips data of any other columns, if present.\\
udf.util.GetMinMax() & Extracts the minimum and maximum value per feature, for the documents of a single query.\\
udf.util.CombineMinMax() & Combines outputs of the udf.util.GetMinMax() UDF for each query into globally minimum and maximum feature values.\\
\bottomrule
\end{tabular}
\caption{Description of preprocessing User Defined Functions (1/2)}
\label{tbl:preprocessing_udfs_1}
\end{table}

The second Pig job:\\
\begin{minipage}{\linewidth}
\begin{lstlisting}
REGISTER [path prefix]/lib/*.jar;
TRAIN = LOAD '[path prefix]/input/[dataset name]/Fold[fold number]/train.txt' USING PigStorage(' ');
VALIDATE = LOAD '[path prefix]/input/[dataset name]/Fold[fold number]/vali.txt' USING PigStorage(' ');
TEST = LOAD '[path prefix]/input/[dataset name]/Fold[fold number]/test.txt' USING PigStorage(' ');
TRAIN_STD = FOREACH TRAIN GENERATE flatten(udf.util.ToStandardForm(*));
VALIDATE_STD = FOREACH VALIDATE GENERATE flatten(udf.util.ToStandardForm(*));
TEST_STD = FOREACH TEST GENERATE flatten(udf.util.ToStandardForm(*));
DEFINE ScaleFeatures udf.util.ScaleFeatures('[array with minimum and maximum feature values]');
TRAIN_SCA = FOREACH TRAIN_STD GENERATE flatten(ScaleFeatures(*));
VALIDATE_SCA = FOREACH VALIDATE_STD GENERATE flatten(ScaleFeatures(*));
TEST_SCA = FOREACH TEST_STD GENERATE flatten(ScaleFeatures(*));
STORE TRAIN_SCA INTO 'train_sca[fold number]' USING BinStorage();
STORE VALIDATE_SCA INTO 'validate_sca[fold number]' USING BinStorage();
STORE TEST_SCA INTO 'test_sca[fold number]' USING BinStorage();
\end{lstlisting}
\end{minipage}\\

\begin{table}
\centering
\begin{tabular}{p{5cm}p{8cm}}\toprule
UDF & Description \\
\midrule
udf.util.ToStandardForm() & See Table \ref{tbl:preprocessing_udfs_1} for description.\\
udf.util.ScaleFeatures() & Uses the minimum and maximum feature values with which it is parametrised perform the following rescaling transformation to the features: $x^{'} = \frac{x-min(x)}{max(x)-min(x)}$.\\
\bottomrule
\end{tabular}
\caption{Description of preprocessing User Defined Functions (2/2)}
\label{tbl:preprocessing_udfs_2}
\end{table}

\subsection{Training}
The training stage, like the preprocessing stage, consists of two separate Pig jobs. The first Pig job calculates the Cross Entropy loss on training data of the current model and calculates the gradients for the next model update. The second Pig job is an internal validation step that validates the model on the validation set by calculating \ac{nDCG}@k.\\

The first Pig job:\\
\begin{minipage}{\linewidth}
\begin{lstlisting}
REGISTER [path prefix]/lib/*.jar;
DEFINE QueryLossGradient udf.listnet.QueryLossGradient('[feature dimensionality of data set]');
DEFINE ExpRelOurScores udf.listnet.ExpRelOurScores('[neural network weights & iteration number]');
[FIRST TRAINING ITERATION:]
	TRAIN_SCA = LOAD 'train_sca[fold number]/*' USING BinStorage();
	TR_BY_QUERY = GROUP TRAIN_SCA BY $1 PARALLEL [number of avaiable reducers];
	TR_EXP_REL_SCORES = FOREACH TR_BY_QUERY GENERATE flatten(ExpRelOurScores(TRAIN_SCA));
	STORE TR_EXP_REL_SCORES INTO 'tr_exp_rel_scores-f[fold number]' USING BinStorage();
[SUBSEQUENT TRAINING ITERATIONS:]
	TR_EXP_REL_SCORES = LOAD 'tr_exp_rel_scores-f[fold number]/*' USING BinStorage();
	TR_EXP_REL_SCORES = FOREACH TR_EXP_REL_SCORES GENERATE flatten(ExpRelOurScores(*)) PARALLEL [number of available reducers];
TR_QUERY_LOSS_GRADIENT = FOREACH TR_EXP_REL_SCORES GENERATE flatten(QueryLossGradient(*)) PARALLEL [number of available reducers];
TR_QUERY_LOSS_GRADIENT_GRPD = GROUP TR_QUERY_LOSS_GRADIENT ALL;
TR_LOSS_GRADIENT = FOREACH TR_QUERY_LOSS_GRADIENT_GRPD GENERATE flatten(udf.listnet.MultiSum(*));
STORE TR_LOSS_GRADIENT INTO 'tr_loss_gradient-f[fold number]i[iteration number]';
\end{lstlisting}
\end{minipage}\\

\begin{table}
\centering
\begin{tabular}{p{6cm}p{7cm}}\toprule
UDF & Description \\
\midrule
udf.listnet.QueryLossGradient() & Calculates the Cross Entropy loss for a query and calculates the gradients per feature based on this query.\\
udf.listnet.ExpRelOurScores() & Calculates the predicted relevance label of a query based on the current model weights and transforms this following the transformation $x -> e^{x}$. In case the current iteration is the first iteration, the same transformation is applied to the ground truth relevance label.\\
udf.listnet.MultiSum() & Calculates aggregated loss and feature gradients by summing the per-query losses and per-query feature gradients.\\
\bottomrule
\end{tabular}
\caption{Description of training User Defined Functions (1/2)}
\label{tbl:training_udfs_1}
\end{table}

The second Pig job of the training stage validates the performance of the model weights that were trained in the first Pig job on the validation data set.\\

The second Pig job:\\
\begin{minipage}{\linewidth}
\begin{lstlisting}
REGISTER [path prefix]/lib/*.jar;
DEFINE Ndcg udf.util.Ndcg('[neural network weights & NDCG cut-off parameter]');
[FIRST TRAINING ITERATION:]
	VALIDATE_SCA = LOAD 'validate_sca[fold number]/*' USING BinStorage();
	VA_BY_QUERY = GROUP VALIDATE_SCA BY $1 PARALLEL [number of available reducers];
	STORE VA_BY_QUERY INTO 'va_by_query-f[fold number]' USING BinStorage();
[SUBSEQUENT TRAINING ITERATIONS:]
	VA_BY_QUERY = LOAD 'va_by_query-f[fold number]/*' USING BinStorage();
NDCG = FOREACH VA_BY_QUERY GENERATE Ndcg(*);
NDCG_GRPD = GROUP NDCG ALL;
AVG_NDCG = FOREACH NDCG_GRPD GENERATE AVG(NDCG);
STORE AVG_NDCG INTO 'avg_ndcg-f[fold number]i[iteration number]';
\end{lstlisting}
\end{minipage}\\

\begin{table}
\centering
\begin{tabular}{p{6cm}p{7cm}}\toprule
UDF & Description \\
\midrule
udf.util.Ndcg() & Calculates \ac{nDCG}@k for a query.\\
\bottomrule
\end{tabular}
\caption{Description of training User Defined Functions (2/2)}
\label{tbl:training_udfs_2}
\end{table}

\subsection{Testing}
The testing stage tests the best model found in the training iterations, selected on validation set \ac{nDCG}@k (as calculated in the second Pig job of the training stage), by calculating the \ac{nDCG}@k of this model on the test set.\\

Test stage Pig job:\\
\begin{minipage}{\linewidth}
\begin{lstlisting}
REGISTER [path prefix]/lib/*.jar;
TEST_SCA = LOAD 'test_sca[fold number]/*' USING BinStorage();
TE_BY_QUERY = GROUP TEST_SCA BY $1 PARALLEL [number of available reducers];
DEFINE Ndcg udf.util.Ndcg('[neural network weights & NDCG cut-off parameter]');
NDCG = FOREACH TE_BY_QUERY GENERATE Ndcg(*);
NDCG_GRPD = GROUP NDCG ALL;
AVG_NDCG = FOREACH NDCG_GRPD GENERATE AVG(NDCG);
STORE AVG_NDCG INTO 'avg_ndcg';
\end{lstlisting}
\end{minipage}\\

\section{SmoothRank}
\subsection{Preprocessing}
\subsection{Training}
\subsection{Testing}