Using our experimental results we will reflect on our research questions stated in the \ref{sec:goals} section of this thesis. We formulated the following research questions:
\begin{description}
\item[RQ1] What are the best performing Learning-to-Rank algorithms in terms of accuracy on relevant benchmark datasets?
\end{description}
To answer this research question we proposed a new way of comparing learning to rank methods based on sparse evaluation results data on a set of benchmark datasets. Our comparison methodology comprises of two components: 1) NWN, which provides insight in the ranking accuracy of the learning to rank method, and 2) IWN, which gives insight in the degree of certainty concerning the performance of the ranking accuracy. Based on our literature search for evaluation results on well-known benchmarks collections, a lot of insight has been gained with the cross-benchmark comparison on which methods tend to perform better than others. However, no closing arguments can be formulated on which learning to rank methods are most accurate. LRUF, FSMRank, FenchelRank, SmoothRank and ListNet were the learning to rank algorithms for which it holds that no other algorithm produced more accurate rankings with a higher degree of certainty of ranking accuracy. From left to right, the ranking accuracy of these methods decreases while the certainty of the ranking accuracy increases. More evaluation runs are needed for the methods on the left side of Figure \ref{fig:normalised_winning_number_all}. Our work contributes to this by identifying promising learning to rank methods that researchers could focus on in performing additional evaluation runs.

\begin{description}
\item[RQ2] What is the speed-up of those Learning-to-Rank algorithms when executed using the MapReduce framework?\\
Where the definition of \emph{relative speed-up} is used for speed-up \cite{Sun1991}:\\

$S_N = \frac{\text{execution time using one core}}{\text{execution time using \emph{N} cores}}$
\end{description}
To answer this research question we took the approach to implementing the list of algorithms found in answering the first research question starting with the Learning to Rank method with the highest certainty on ranking accuracy, ListNet. We found that running ListNet on a Hadoop cluster using the MapReduce computing model comes with its own cost in the form of a job scheduling overhead in the range of 150-200 seconds per training iteration. However, with a normalisation preprocessing iteration we can greatly reduce the number of training iterations needed for converge. ListNet on a single machine does not scale well to data sizes larger than the physical memory size. To process data sets in the 100+GB range with the ListNet training algorithm, Hadoop MapReduce is a large improvement compared to a single machine.\\
% TODO: iets zeggen over waarom de oorspronkelijk speed-up metric niet zo geschikt is hier %
No generalisations can be drawn from this results to other Learning to Rank algorithms. Although we can extend our findings on job scheduling overhead and scaling benefit from data larger than 100+GB to the gradient descent procedure that is used in ListNet as well as in many other Learning to Rank algorithms and in many learning algorithms in general. Other Learning to Rank algorithms using the gradient descent procedure might not scale well when the MapReduce computing model is used, but any bad scaling behaviour on MapReduce of Learning to Rank algorithms will not be because of gradient descent.

