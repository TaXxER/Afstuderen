Using our experimental results we will now reflect on our research questions stated in the \ref{sec:goals} section of this thesis. We formulated the following research questions:
\begin{description}
\item[RQ1] What are the best performing Learning to Rank algorithms in terms of ranking accuracy on relevant benchmark data sets?
\end{description}
To answer this research question we proposed a new way of comparing learning to rank methods based on sparse evaluation results data on a set of benchmark datasets. Our comparison methodology comprises of two components: 1) \ac{NWN}, which provides insight in the ranking accuracy of the learning to rank method, and 2) \ac{IWN}, which gives insight in the degree of certainty concerning the performance of the ranking accuracy. Based on our literature search for evaluation results on well-known benchmarks collections, insight has been gained with the cross-benchmark comparison on which methods tend to perform better than others. However, no closing arguments can be formulated on which learning to rank methods are most accurate. LRUF, FSMRank, FenchelRank, SmoothRank and ListNet were the learning to rank algorithms for which it holds that no other algorithm produced more accurate rankings with a higher degree of certainty of ranking accuracy. From left to right, the ranking accuracy of these methods decreases while the certainty of the ranking accuracy increases. More evaluation runs are needed to increase the certainty of the ranking accuracy of the methods that were found to have low \ac{IWN} values. Our work contributes to this by identifying promising learning to rank methods that researchers could focus on in performing additional evaluation runs.

\begin{description}
\item[RQ2] What is the speed-up of those Learning to Rank algorithms when executed using the MapReduce framework?
\end{description}
\bigskip
Where the definition of \emph{relative speed-up} is used for speed-up \cite{Sun1991}:\\

$S_N = \frac{\text{execution time using one core}}{\text{execution time using \emph{N} cores}}$\\

To answer this research question, we took the approach of implementing the list of algorithms found in answering the first research question, starting with the Learning to Rank method with the highest certainty on ranking accuracy, ListNet.\\

We found that running ListNet on a Hadoop cluster using the MapReduce computing model comes with its own cost in the form of a job scheduling overhead in the range of 150-200 seconds per training iteration. This makes Hadoop very inefficient for the processing of small data sets, where the Hadoop overhead tends to make up a large share of the total processing time. For small data sets where the constant 150-200 seconds job scheduling overhead is a large fraction of the total processing time, single-machine computation, which does not have this job scheduling overhead, is found to be faster than Hadoop MapReduce computation. For large data sets, where 150-200 seconds overhead per iteration is small compared to the total time that it would take to process the data, Hadoop MapReduce can provide a speed-up to the training process. ListNet on a single machine does not scale well to data sizes larger than the physical memory size. To process large data sets with the ListNet training algorithm, Hadoop MapReduce is a large improvement compared to a single machine.\\

Moreover, we found that the addition of a normalisation preprocessing step to the ListNet procedure can greatly improve the ranking accuracy of the ListNet training procedure after the first five iterations. This suggests that the addition of a normalisation preprocessing step can reduce the number of training iterations needed for convergence. Lin stated in his essay \cite{Lin2013} that MapReduce is often good enough for tasks that are not-amenable to the MapReduce model. Lin motivates this statement in the context of iterative algorithms with the observation that these iterative algorithms can often be optimised in such a way that less iterations are needed for convergence. Our preprocessing step can improve the convergence rate of the ListNet training iteration, and therefore fits into Lin's point of view.\\

Most importantly, we found the training time of our cluster version of ListNet to grow better than linearly in terms of data size increase. This shows that the cluster implementation of ListNet can be used to scale the ListNet training procedure to arbitrarily large data sets, given that enough data nodes are available for computation.\\

No generalisations can be drawn from these results to the scalability on MapReduce of other Learning to Rank algorithms. However, we can extend our findings on job scheduling overhead and scaling benefit on large data sets to the gradient descent procedure that is used in ListNet as well as in many other Learning to Rank algorithms and in many learning algorithms in general. Other Learning to Rank algorithms using the gradient descent procedure might not scale well when the MapReduce computing model is used, but any bad scaling behaviour on MapReduce of Learning to Rank algorithms will not be caused by bad scaling of the gradient descent procedure.

