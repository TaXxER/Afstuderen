Running ListNet on a Hadoop cluster using the MapReduce computing model comes with its own cost in the form of a job scheduling overhead in the range of 150-200 seconds per training iteration. However, with a normalisation preprocessing iteration we can greatly reduce the number of training iterations needed for converge. ListNet on a single machine does not scale well to data sizes larger than the physical memory size. To process data sets in the 100+GB range with the ListNet training algorithm, Hadoop MapReduce is a large improvement compared to a single machine.\\

No generalisations can be drawn from this results to other Learning to Rank algorithms. Although we can extend our findings on job scheduling overhead and scaling benefit from data larger than 100+GB to the gradient descent procedure that is used in ListNet as well as in many other Learning to Rank algorithms and in many learning algorithms in general. Other Learning to Rank algorithms using the gradient descent procedure might not scale well when the MapReduce computing model is used, but any bad scaling behaviour on MapReduce of Learning to Rank algorithms will not be because of gradient descent.

% TODO: how does this generalise to other Learning to Rank algorithms %