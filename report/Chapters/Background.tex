This chapter provides an introduction to Learning to Rank and MapReduce. Knowledge about the models and theories explained in this chapter is required to understand the subsequent chapters of this thesis.
\section{A basic introduction to Learning to Rank}
Different definitions of Learning to Rank exist. In general, all ranking methods that use machine learning technologies to solve the problem of ranking are called Learning to Rank methods. Figure \ref{fig:discriminative_training} describes the general process of machine learning. Input space $X$ consists of input objects $x$. A hypothesis $h$ defines a mapping of input objects from $X$ into the output space $Y$, resulting in prediction $\hat{y}$. The loss of an hypothesis is the difference between the predictions made by the hypothesis and the correct values mapped from the input space into the output space, called the \emph{ground truth labels}. The task of machine learning is to find the best fitting hypothesis $h$ from the set of all possible hypotheses $H$, called the hypothesis space.\\

\begin{figure}[!h]
\centering
\includegraphics[scale=0.36]{gfx/descriminative_training}
\caption{Machine learning framework for Learning to Rank, obtained from Liu \cite{Liu2007}}
\label{fig:discriminative_training}
\end{figure}

Liu \cite{Liu2007} proposes a more narrow definition and only considers ranking methods to be a Learning to Rank method when it is \emph{feature based} and uses \emph{discriminative training}, in which the concepts \emph{feature-based} and \emph{discriminative training} are themselves defined as:
\begin{description}
\item[Feature-based]{means that all objects under investigation are represented by feature vectors. In a Learning to Rank for Information Retrieval case, this means that the feature vectors can be used to predict the relevance of the documents to the query, or the importance of the document itself.}
\item[Discriminative Training]{means that the learning process can be well described by the four components of discriminative learning. That is, a Learning to Rank method has its own \emph{input space}, \emph{output space}, \emph{hypothesis space}, and \emph{loss function}, like the machine learning process described by Figure \ref{fig:discriminative_training}. \emph{Input space}, \emph{output space}, \emph{hypothesis space}, and \emph{loss function} are hereby defined as follows:
	\begin{description}
	\item[Input Space]{contains the objects under investigation. Usually objects are represented by feature vectors, extracted from the objects themselves.}
	\item[Output Space]{contains the learning target with respect to the input objects.}
	\item[Hypothesis Space]{defines the class of functions mapping the input space to the output space. The functions operate on the feature vectors of the input object, and make predictions according to the format of the output space.}
	\item[Loss Function]{in order to learn the optimal hypothesis, a training set is usually used, which contains a number of objects and their ground truth labels, sampled from the product of the input and output spaces. A loss function calculates the difference between the predictions $\hat{y}$ and the ground truth labels on a given set of data.}
	\end{description}
	}
\end{description}


Figure \ref{fig:ltr_framework} shows how the machine learning process as described in Figure \ref{fig:discriminative_training} typically takes place in a ranking scenario. Let $q_i$ with $1 \le i \le n$ be a set of queries of size $n$. Let $x_j^i$ with $1 \le j \le m$ be the sets of documents of size $m$ that are associated with query $i$, in which each document is represented by a feature vector. The queries, the associated documents and the relevance judgements $y_i$ are jointly used to train a model $h$. Model $h$ can after training be used to predict a ranking of the documents for a given query, such the difference between the document rankings predicted by $h$ and the actual optimal rankings based on $y_i$ is minimal in terms of a certain loss function.
\begin{figure}[!h]
\centering
\includegraphics[scale=0.36]{gfx/ltr_framework}
\caption{A typical Learning to Rank setting, obtained from Liu \cite{Liu2007}}
\label{fig:ltr_framework}
\end{figure}\\

Learning to Rank algorithms can be divided into three groups: the pointwise approach, the pairwise approach and the listwise approach. The approaches are explained in more detail in section \ref{sec:ltr_approaches}. The main difference between the three approaches is in the way in which they define the input space and the output space. 
\begin{description}
\item[Pointwise] the relevance of each associated document
\item[Pairwise] the classification of the most relevant document out for each pair of documents in the set of associated documents
\item[Listwise] the relevance ranking of the associated documents
\end{description}
\smallskip

\section{How to evaluate a ranking}
\label{sec:how_to_evaluate_a_ranking}
Evaluation metrics have long been studied in the field of information retrieval. First in the form of evaluation of unranked retrieval sets and later, when the information retrieval field started focussing more on ranked retrieval, in the form of ranked retrieval evaluation. In this section several frequently used evaluation metrics for ranked results will be described.\\

No single evaluation metric that we are going to describe is indisputably better or worse than any of the other metrics. Different benchmarking settings have used different evaluation metrics. Metrics introduced in this section will be used in chapters \ref{chap:benchmark_results} and \ref{chap:cross_comparison} of this thesis to compare Learning to Rank methods in terms of ranking accuracy.
\subsection{Normalized Discounted Cumulative Gain}
Cumulative gain and its successors discounted cumulative gain and normalized discounted cumulative gain are arguably the most widely used measures for effectiveness of ranking methods. Cumulative Gain, without discounting factor and normalisation step, is defined as\\

$CG_{k} = \sum\nolimits_{i=1}^k rel_i$

\subsubsection{Discounted Cumulative Gain}
There are two definitions of \ac{DCG} used in practice. \ac{DCG} for a predicted ranking of length $p$ was originally defined by J{\"a}rvelin and Kek{\"a}l{\"a}inen \cite{Jarvelin2002} as\\

$DCG_{JK} = \sum\nolimits_{i=1}^p \frac{rel_i-1}{log_2(i+1)}$\\

with $rel_i$ the graded relevance of the result at position $i$. The idea is that highly relevant documents that appear lower in a search result should be penalized (discounted). This discounting is done by reducing the graded relevance  logarithmically proportional to the position of the result.\\

Burges et al. \cite{Burges2005} proposed an alternative definition of \ac{DCG} that puts stronger emphasis on document relevance\\

$DCG_{B} = \sum\nolimits_{i=1}^p \frac{2^{rel_i-1}}{log_2(i+1)}$\\

\subsubsection{Normalized Discounted Cumulative Gain}
\ac{NDCG} normalizes the \ac{DCG} metric to a value in the [0,1] interval by dividing by the \ac{DCG} value of the optimal rank. This optimal rank is obtained by sorting documents on relevance for a given query. The definition of \ac{NDCG} can be written down mathematically as\\

$NDCG = \frac{DCG}{IDCG}$\\

Often it is the case that queries in the data set differ in the number of documents that are associated with them. For queries with a large number of associated documents it might not always be needed to rank the complete set of associated documents, since the lower sections of this ranking might never be examined. \acl{NDCG} is often used with a fixed set size for the result set to mitigate this problem. \ac{NDCG} with a fixed set size is often called \ac{NDCG}@k, where $k$ represents the set size.\\

Table \ref{tab:example_calculation_NDCG} shows an example calculation for \ac{NDCG}@k with $k=10$ for both the J{\"a}rvelin and Kek{\"a}l{\"a}inen \cite{Jarvelin2002} and Burges et al. \cite{Burges2005} version of \ac{DCG}.\\

\begin{table}[!h]
\begin{tabular}{llllllllllll}
 & \multicolumn{10}{c}{Rank} &  \\ 
\cline{2-11}
 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & Sum \\ 
\hline
\hline
$rel_i$ & 10 & 7 & 6 & 8 & 9 & 5 & 1 & 3 & 2 & 4 &  \\
\hline
$\frac{2^{rel_i-1}}{log_2(i+1)}$ & 512 & 40.4 & 16 & 55.1 & 99.0 & 5.7 & 0.3 & 1.3 & 0.6 & 2.3 & 732.7 \\
\hline
$\frac{rel_i}{log_2(i+1)}$ & 10 & 4.42 & 3 & 3.45 & 3.48 & 1.78 & 0.33 & 0.95 & 0.6 & 1.16 & 29.17 \\  
\hline
\hline
optimal rank & 10 & 9 & 8 & 7 & 6 & 5 & 4 & 3 & 2 & 1 &  \\
\hline 
$\frac{2^{rel_i-1}}{log_2(i+1)}$ & 512 & 161.5 & 64 & 27.6 & 12.4 & 5.7 & 2.7 & 1.3 & 0.6 & 0.2 & 788.0 \\
\hline
$\frac{rel_i}{log_2(i+1)}$ & 10 & 5.68 & 4 & 3.01 & 2.32 & 1.78 & 1.33 & 0.95 & 0.6 & 0.29 & 29.96 \\   
\hline
 &  &  &  &  &  &  &  &  &  &  &  \\
  & \multicolumn{10}{c}{$NDCG_{B}@10$ = $\frac{732.7}{788.0} = 0.9298$} &  \\ 
  & \multicolumn{10}{c}{$NDCG_{JK}@10$ = $\frac{29.17}{29.96} = 0.9736$} &  \\
\end{tabular}
\caption{Example calculation of \acs{NDCG}@10}
\label{tab:example_calculation_NDCG}
\end{table}

\subsection{Expected Reciprocal Rank}
\ac{ERR} \cite{Chapelle2009} was designed based on the observation that \ac{NDCG} is based on the false assumption that the usefulness of a document at rank $i$ is independent of the usefulness of the documents at rank less than $i$. \ac{ERR} is based on the reasoning that a user examines search results from top to bottom and at each position has a certain probability of being satisfied in his information need, at which point he stops examining the remainder of the list. The \ac{ERR} metric is defined as the expected reciprocal length of time that the user will take to find a relevant document. \ac{ERR} is formally defined as\\

$ERR = \sum\nolimits_{r=1}^n \frac{1}{r} \prod\nolimits_{i=1}^{r-1}(1-R_i)R_r$\\

where the product sequence part of the formula represents the chance that the user will stop at position $r$. $R_i$ in this formula represents the probability of the user being satisfied in his information need after assessing the document at position $i$ in the ranking.\\

The algorithm to compute \ac{ERR} is shown in Listing \ref{alg:err}. The algorithm requires relevance grades $g_i$, $1 \le i \le n$ and mapping function $R$ that maps relevance grades to probability of relevance.\\

\LinesNumbered
\begin{algorithm}[H]
 $p \leftarrow 1, ERR \leftarrow 0$\\
 \For{$r\leftarrow 1$ \KwTo $n$}{
	$R \leftarrow \mathcal{R}(rel_r)$\\
	$ERR \leftarrow ERR + p * \frac{R}{r}$\\
	$p \leftarrow p * (1-R)$
 }
 Output $ERR$
 \caption{The algorithm for computation of the \acs{ERR} metric, obtained from Chapelle et al. \cite{Chapelle2009}}
 \label{alg:err}
\end{algorithm}
\vspace{0.5cm}
In this algorithm $\mathcal{R}$ is a mapping from relevance grades to the probability of the document satisfying the information need of the user. Chapelle et al. \cite{Chapelle2009} state that there are different ways to define this mapping, but they describe one possible mapping that is based on the Burges version \cite{Burges2005} of the gain function for \ac{DCG}:\\

$\mathcal{R}(r) = \frac{2^{r}-1}{2^{max\_rel}}$\\

\noindent where $max\_rel$ is the maximum relevance value present in the data set.

\subsection{Mean Average Precision}
\ac{AP} \cite{Zhu2004} is an often used binary relevance-judgement-based metric that can be seen as a trade-off between precision and recall that is defined as\\

$AP(q) = \frac{\sum\nolimits_{k=1}^{n}Precision(k)*rel_k}{\text{number of relevant docs}}$\\

where $n$ is the number of documents in query $q$. Since \ac{AP} is a binary relevance judgement metric, $rel_k$ is either 1 (relevant) or 0 (not relevant). Table \ref{tab:example_calculation_AP} provides an example calculation of average precision where de documents at positions 1, 5, 6 and 8 in the ranking are relevant. The total number of available relevant documents in the document set \emph{R} is assumed to be seven. \ac{MAP} is the average \ac{AP} for a set of queries.\\

$MAP = \frac{\sum\nolimits_{q=1}^{Q}AP(q)}{Q}$\\

In this formula $Q$ is the number queries.

\begin{table}
\begin{tabular}{lllllllllllll}
 & \multicolumn{10}{c}{Rank} &  & Sum \\ 
\cline{2-11}
 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 &  &  \\ 
\hline
$r_i$ & 1 & 0 & 0 & 0 & 1 & 1 & 0 & 1 & 0 & 0 &  &  \\ 
P@$i$ & 1 &  &  &  & 0.4 & 0.5 &  & 0.5 &  &  &  & 2.4 \\ 
\hline
 &  &  &  &  &  &  &  &  &  & \# of relevant docs & = & 7 \\ 
 &  &  &  &  &  &  &  &  &  & AP@10 & = & 0.34 \\ 
\end{tabular}
\caption{Average Precision example calculation.}
\label{tab:example_calculation_AP}
\end{table}

\section{Approaches to Learning to Rank}
\label{sec:ltr_approaches}
\subsection{Pointwise Approach}
The pointwise approach can be seen as the most straightforward way of using machine learning for ranking. Pointwise Learning to Rank methods directly apply machine learning methods to the ranking problem by observing each document in isolation. They can be subdivided in the following approaches:
	\begin{enumerate}
	\item regression-based, which estimate the relevance of a considered document using a regression model.
	\item classification-based, which classify the relevance category of the document using a classification model.
	\item ordinal regression-based, which classify the relevance category of the document using a classification model in such a way that the order of relevance categories is taken into account. 
	\end{enumerate}
Well-known algorithms that belong to the pointwise approach include McRank \cite{Li2007} and PRank \cite{Crammer2001}.
\subsection{Pairwise Approach}
Pointwise Learning to Rank methods have the drawback that they optimise real-valued expected relevance, while evaluation metrics like \ac{NDCG} and \ac{ERR} are only impacted by a change in expected relevance when that change impacts a pairwise preference. The pairwise approach solves this drawback of the pointwise approach by regarding ranking as pairwise classification.\\

Aggregating a set of predicted pairwise preferences into the corresponding optimal rank is shown to be a NP-Hard problem \cite{Feldman2012}. An often used solution to this problem is to upper bound the number of classification mistakes by an easy to optimise function \cite{Bartlett2006}.\\

Well-known pairwise Learning to Rank algorithms include FRank \cite{Tsai2007}, GBRank \cite{Zheng2007}, LambdaRank \cite{Burges2006}, RankBoost \cite{Freund2003}, RankNet \cite{Burges2005}, Ranking \acs{SVM} \cite{Herbrich1999b,Joachims2002}, and SortNet \cite{Rigutini2008}.
\subsection{Listwise Approach}
Listwise ranking optimises the actual evaluation metric. The learner learns to predict an actual ranking itself without using an intermediate step like in pointwise or pairwise Learning to Rank. The main challenge in this approach is that most evaluation metrics are not differentiable. \ac{MAP}, \ac{ERR} and \ac{NDCG} are non-differentiable, non-convex and discontinuous functions, what makes them very hard to optimize.\\

Although the properties of \ac{MAP}, \ac{ERR} and \ac{NDCG} are not ideal for direct optimisation, some listwise approaches do focus on direct metric optimisation \cite{Yue2007, Taylor2008, Chapelle2010}. Most listwise approaches work around optimisation of the non-differentiable, non-convex and discontinuous metrics by optimising surrogate cost functions that mimic the behaviour of \ac{MAP}, \ac{ERR} or \ac{NDCG}, but have nicer properties for optimisation.\\

Well-known algorithms that belong to the listwise approach include AdaRank \cite{Xu2007}, BoltzRank \cite{Volkovs2009}, ListMLE \cite{Xia2008}, ListNet \cite{Cao2007}, RankCosine \cite{Qin2008}, SmoothRank \cite{Chapelle2010}, SoftRank \cite{Taylor2008}, \acs{SVM}$^{map}$ \cite{Yue2007}.

\section{Cross-validation experiments}
\label{sec:cross_validation}
A cross-validation experiment \cite{kohavi1995}, sometimes called rotation estimation, is an experimental set-up for evaluation where the data is split into $k$ chunks of approximately equal size, called \emph{folds}. One of the folds is used as validation set, one of the folds is used as test set, and the rest of the $k - 2$ folds are used as training data. This procedure is repeated $k$ times, such that each fold is once used for validation, once as test set, and $k - 2$ times as training data. The performance can be measured in any model evaluation metric, and is averaged over the model performances on each of the folds. The goal of cross-validation is to define a data set to test the model in the training phase, in order to limit the problem of overfitting.\\

Cross-validation is one of the most frequently used model evaluations methods in the field of Machine Learning, including the Learning to Rank subfield. Often, folds in a cross-validation are created in a \emph{stratified} manner, meaning that the folds are created in such a way that the distributions of the target variable are approximately identical between the folds.

\section{An introduction to the MapReduce programming model}
MapReduce \cite{Dean2004} is a programming model invented at Google, where users specify a \emph{map} function that processes a key/value pair to generate a set of intermediate key/value pairs, and a \emph{reduce} function that merges all intermediate values associated with the same intermediate key. This model draws its inspiration from the field of functional programming, where \emph{map} and \emph{reduce} (in some functional languages called \emph{fold}) are commonly used functions.\\

This combination of the \emph{map} and \emph{reduce} functions allows for parallel computation. In the \emph{map} phase parallel computation can be performed by simply splitting the input data after a certain number of bytes, where each worker nodes performs the user-specified \emph{map}-function on its share of the data. Before the \emph{reduce} phase these intermediate answers of the different worker nodes are transformed in such a way that they are grouped by key value, this is called the shuffle-phase. After the shuffle-phase, the user-defined \emph{reduce}-function is applied to each group of key/value pairs in the reduce phase. Since the key/value pairs are already grouped by key in the shuffle phase, this \emph{reduce}-function can be applied to a group of key/value pairs on any of the worker nodes.\\