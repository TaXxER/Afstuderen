%********************************************************************
% Appendix
%*******************************************************
% If problems with the headers: get headings in appendix etc. right
%\markboth{\spacedlowsmallcaps{Appendix}}{\spacedlowsmallcaps{Appendix}}
\chapter{Motivation and Problem Statement}
Ranking is a core problem in the field of information retrieval. The ranking task in information retrieval entails the ranking of candidate documents according to their relevance for a query. Research in the field of ranking models has for a long time been based on manually designed ranking functions and has been an active area of research since Luhn\cite{Luhn1957} was the first to propose a model that assigned relevance scores to documents given a query back in 1957. The increasing amounts of potential training data have recently made it possible to leverage machine learning methods to obtain more effective models. Learning-to-Rank is the relatively new research area covering the use of machine learning models for the ranking task.\\

In recent years several Learning-to-Rank benchmark datasets have been proposed that enable comparison of the performance of different Learning-to-Rank methods. Well-known benchmark datasets include the \emph{Yahoo! Learning to Rank Challenge} dataset\cite{Chapelle2011a}, the Yandex Internet Mathematics competition\footnote{http://imat-relpred.yandex.ru/en/}, and the LETOR dataset\cite{Qin2010} that was build by Microsoft Research. One of the concluding observations of the \emph{Yahoo! Learning to Rank Challenge} was that almost all work in the Learning-to-Rank field focuses on ranking accuracy, while efficiency and scalability of Learning-to-Rank algorithms is still an underexposed research area that is likely to become more important in the near future as training sets are becoming larger and larger\cite{Chapelle2011b}. Liu\cite{Liu2007} confirms the observation that efficiency and scalability of Learning-to-Rank methods has so far been an overlooked research area in his influential book on Learning-to-Rank.\\

Some research has been done in the area of parallel or distributed machine learning \cite{Chu2007,Chang2007}. However, almost none of these studies include the Learning-to-Rank sub-field of machine learning. The field of efficient Learning-to-Rank has been getting some attention lately \cite{Asadi2013a,Asadi2013b,Busa-Fekete2012,Sousa2012,Shukla2012}, since Liu \cite{Liu2007} first stated its growing importance back in 2007. Only several of these studies \cite{Sousa2012,Shukla2012} have explored the possibilities of efficient Learning-to-Rank through the use of parallel programming paradigms.\\

MapReduce\cite{Dean2004} is a parallel programming framework that is inspired by the \emph{Map} and \emph{Reduce} functions commonly used in functional programming. Since Google developed the MapReduce parallel programming framework back in 2004 it has since grown to be the industry standard model for parallel programming. Lin \cite{Lin2013} observed that algorithms that are of iterative nature, which most Learning-to-Rank algorithms are, are not amenable to the MapReduce framework. Lin argued that as a solution to the non-amenability of iterative algorithms to the MapReduce framework, iterative algorithms can often be replaced with non-iterative alternatives or can still be optimized in such a way that its performance in a MapReduce setting is good enough.\\

The appearance of benchmark datasets gave insight in the performance of different Learning-to-Rank approaches, which resulted in increasing popularity of those methods that showed to perform well on one or more benchmark datasets. Up to now it remains unknown whether popular existing Learning-to-Rank methods scale well when they are used in a parallel manner using the MapReduce framework. This thesis aims to be an exploratory start in this little researched area of parallel Learning-to-Rank. A more extensive overview of my research goals and questions are described in chapter \ref{chap:goals}.\\

\chapter{Research Goals}
\label{chap:goals}
The objective of this thesis is to explore the speed-up in execution time of Learning-to-Rank algorithms through parallelisation using the MapReduce framework. 
This work focuses on those Learning-to-Rank algorithms that have shown leading performance on relevant benchmark datasets.
The research questions raised and answered in this work include:
\begin{itemize}
\item What is the speed-up of existing Learning-to-Rank algorithms when executed using the MapReduce framework?\\
In which Nussbaum and Agarwal's definition of asymptotic speed-up will be used for speed-up $S(s)$\cite{Nussbaum1991}:\\

\begin{math}
s \equiv \text{problem size}\\
T_{seq}(s) \equiv \Theta(\text{Serial Running Time})\\
T_{par}(s) \equiv \Theta(\text{Minimum Parallel Running Time})\\
S(s) \equiv \frac{T_{seq}(s)}{T_{par}(s)}\\
\end{math}

\item Can we adjust those Learning-to-Rank algorithms such that the parallel execution speed-up increases without decreasing accuracy?
\end{itemize}

\chapter{Approach}
A literature study will be performed to get insight in relevant existing techniques for large scale Learning-to-Rank. The literature study will be performed by using the following query:
\begin{itemize}
\item ("learning to rank" OR "learning-to-rank" OR "machine learned ranking") AND ("large scale" OR "parallel" OR "distributed")
\end{itemize}
and the following bibliographic databases:
\begin{itemize}
\item Scopus
\item Web of Science
\end{itemize}

To answer the first research question I will implement Learning-to-Rank methods in the MapReduce framework and measuring the runtime as a factor of the number of cluster nodes used to complete the computation.\\

To implement the Learning-to-Rank algorithms I will use cloud based MapReduce implementation from Microsoft was used that is called HDInsight. It which is based on the popular MapReduce open source implementation Hadoop\footnote{http://hadoop.apache.org/}.
The algorithms that we include in the measurements will be determined based on experimental results on the \emph{Yahoo! Learning to Rank Challenge}\cite{Chapelle2011a}, the Yandex Internet Mathematics competition\footnote{http://imat-relpred.yandex.ru/en/}, the LETOR\cite{Qin2010} dataset and the LETOR successor MSLR-WEB30k.

\chapter{Thesis Overview}

\begin{description}
\item[Part II: Background]{introduces the reader to the basic principles and recent work in the fields of Learning-to-Rank.}
\item[Part III: Related Work]{concisely describes existing work in the field of parallel machine learning and parallel Learning-to-Rank.}
\item[Part IV: Benchmark Results]{sketches the performance of existing Learning-to-Rank methods on several benchmark datasets and describes the selection of Learning-to-Rank methods for the parallelisation experiments.}
\item[Part V: Selected Learning-to-Rank Methods]{describes the algorithms and details of the selected Learning-to-Rank methods.}
\item[Part VI: Implementation]{describes implementation details of the Learning-to-Rank algorithms in the Hadoop framework.}
\item[Part VII: Results \& Discussion]{presents and discusses speed-up results for the implemented Learning-to-Rank methods.}
\item[Part VIII: Conclusion]{summarizes the results and answers our research questions based on the results. The limitations of our research as well as future research directions in the field are mentioned here.}
\end{description}