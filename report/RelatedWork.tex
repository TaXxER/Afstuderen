\chapter{Related Work}
\section{Search characteristics}
The literature research is performed by using the bibliographic databases Scopus and Web of Science with the following search query: \emph{("learning to rank" OR "learning-to-rank" OR "machine learned ranking") AND ("parallel" OR "distributed")}. An abstract based manual filtering step is applied where I filter those results that actually use the \emph{parallel} or \emph{distributed} terms in context to the \emph{learning to rank}, \emph{learning-to-rank} or \emph{machine learned ranking}. Studies focusing on efficient query evaluation instead of efficient model training are likely to meet all criteria listed. As a last step I will filter out studies based on the whole document that only focus on efficient query evaluation and not on parallel or distributed learning of ranking functions.
\subsection{Scopus}
The defined search query resulted in 65 documents. Only 14 of those documents used \emph{large scale}, \emph{parallel} or \emph{distributed} terms in context to the \emph{learning to rank}, \emph{learning-to-rank} or \emph{machine learned ranking}. 10 out of those 14 documents focussed on parallel or distributed learning of ranking functions.
\subsection{Web of Science}
The defined search query resulted in 16 documents. Four of those documents were also present in the set of 65 documents found using Scopus, leaving 61 unique documents. Only four of those 61 documents used \emph{large scale}, \emph{parallel} or \emph{distributed} terms in context to the \emph{learning to rank}, \emph{learning-to-rank} or \emph{machine learned ranking}, none of them focussed on parallel or distributed learning of ranking functions.
\subsection{Google Scholar}
The defined search query resulted in 3300 documents. We evaluate the first 300 search results.
\section{CCRank}
Wang et al \cite{Wang2011a,Wang2011b} propose a parallel evolutionary algorithm based on \ac{CC}, a type of evolutionary algorithm. The \ac{CC} algorithm is capable of directly optimizing non-differentiable functions, as \ac{nDCG}, in contrary to many optimization algorithms.  the divide-and-conquer nature of the \ac{CC} algorithm enables parallelisation. CCRank showed an increase in both accuracy and efficiency on the LETOR 4.0 benchmark dataset compared to the baselines. It must be stated however that the increased efficiency was achieved through speed-up and not scale-up. Two reasons have been identified for not achieving linear scale-up with CCRank: 1) parallel execution is suspended after each generation to perform combination in order to produce the candidate solution, 2) Combination has to wait until all parallel tasks have finished, which may spend different running time.
\section{Parallel ListNet using Spark}
Shukla et al \cite{Shukla2012} explored the parallelisation of the well-known ListNet Learning-to-Rank method using Spark. Spark is a parallel computing model that is designed for cyclic data flows which makes it more suitable for iterative algorithms. Spark is incorporated into Hadoop since Hadoop 2.0. The Spark implementation of ListNet showed near a linear training time reduction.
\section{Gradient Boosted Distributed Decision Trees}
Ye et al \cite{Ye2009} described how to implement the \ac{GBDT} process in a parallel manner using both MPI and Hadoop. \ac{GBDT}'s are shown to be able to achieve good accuracy in a Learning-to-Rank setting when used in a pairwise \cite{Zheng2007} or listwise \cite{Chen2008} setting. Experiments showed the Hadoop implementation to result into too expensive communication cost to be useful. Authors believed that these high communication costs were a result of the communication intensive implementation that was not well suited for the MapReduce paradigm. The MPI approach proved to be successful and obtained near linear speed-ups.
\section{nDCG-Annealing}
Karimzadeghan el al \cite{Karimzadehgan2011} proposed a method using Simulated Annealing along with the Simplex method for its parameter search. This method directly optimises the often non-differentiable Learning-to-Rank evaluation metrics like \ac{nDCG} and \ac{MAP}. The authors successfully parallelised their method in the MapReduce paradigm using Hadoop. The approach showed to be effective on both the LETOR 3.0 dataset and their own dataset with contextual advertising data. Unfortunately their work does not directly report on the speed-up obtained by parallelising  with Hadoop, but it is mentioned that further work needs to be done to effectively leverage parallel execution.
\section{Low complexity Learning-to-Rank}
Designing Learning-to-Rank algorithms with low time complexity for training is a different another approach towards large scale Learning-to-Rank. Pahikkala et al \cite{Pahikkala2009} describe a pairwise \ac{RLS} type of ranking function, RankRLS, with time complexity $\mathcal{O}(n^3+n^2m)$, with $n$ the number of features and $m$ the number of training documents. The RankRLS ranking function showed ranking performance similar to RankSVM on the BioInfer corpus \cite{Pyysalo2007}, a corpus for information extraction in the biomedical domain.
\section{Distributed Stochastic Gradient Descent}
Long et al \cite{Long2012} describe special case of their pairwise cross-domain factor Learning-to-Rank method using distributed optimization of \ac{SGD} based on Hadoop MapReduce. Parallelisation of the \ac{SGD} optimization algorithm was performed using the MapReduce based method described by  Zinkevich et al \cite{Zinkevich2010} has been used. Real world data from Yahoo! has been used to show that the model is effective. Unfortunately the speed-up obtained by training their model in parallel is not reported.
\section{FPGA-based LambdaRank}
Yan et al \cite{Yan2009,Yan2010,Yan2011,Yan2012} described the development and incremental improvement of a \ac{SIMD} architecture \ac{FPGA} designed to run the Neural Network based LambdaRank Learning-to-Rank algorithm. This architecture achieved a 29.3X speed-up compared to the software implementation when evaluated on data from a commercial search engine. The exploration of \ac{FPGA} for Learning-to-Rank showed other advantages of the \ac{FPGA} approach next to faster model training. In their latest publication \cite{Yan2012} the \ac{FPGA} based LambdaRank implementation showed it could achieve up to 19.52X power efficiency and 7.17X price efficiency for query processing compared to Intel Xeon servers currently used at the commercial search engine.
\section{GPGPU for Learning-to-Rank}
De Sousa et al \cite{DeSousa2012} proposed a \ac{GPGPU} approach using the \ac{GPU} both learning the ranking function and for query processing, thereby improving both training time and query time. An association rule based Learning-to-Rank approach proposed by \cite{Veloso2008} has been implemented using the \ac{GPU} in such a way that the set of rules van be computed in parallel, in different threads, for each document. A speed-up of 127X in query processing time is reported based on evaluation on the LETOR dataset. The speed-up achieved at learning the ranking function was unfortunately not stated.
\section{BagBoo: A hybrid Bagging-the-Boosting Model}
Pavlov et al \cite{Pavlov2010} at Yandex developed a hybrid model that combines two methods from ensemble learning: bagging and boosting. Bagging \cite{Breiman1996}, also called bootstrap aggregating, is a ensemble learning method in which $m$ training sets $D_1..D_m$ are constructed from the training set $D$ by uniformly sampling data items from $D$ with replacement. The bagging method is parallelisable by training each $D_i$ with $i \in \{1..m\}$ on a different node. Boosting \cite{Friedman2002} is an ensemble learning method in which multiple weak learners are iteratively added together into one ensemble model in such a way that new models focus more on those data instances that were misclassified before. Boosting is not parallelisable, but the authors state that learning a short boosted sequence on a single node is still a do-able task.
\section{BoostingTree}
Kocsis et al \cite{Kocsis2013} propose way to train multiple weak learners in parallel and extend those models that are likely to yield a good model when combined through boosting. Authors showed through theoretical analysis that the proposed algorithm asymptotically achieve equal performance to regular boosting. Using this parallel BoostingTree algorithm \ac{GBDT} models could be trained in parallel.
\section{Deep Stacking Networks}
A \ac{DSN} is a processing architecture developed from the field of \emph{Deep Learning}, that uses the \ac{MSE} loss function that is easily optimisable.\ac{DSN} is based on the stacked generalization ensemble learning method \cite{Wolpert1992}, an approach where several learning models are stacked on top of each other in such a way that the output of a model is used as one of the input features of models higher up in the stack. Each layer in the stack learns has the task of learning the weights of the inputs of that layer. The close-form constraints between input and output weights allow the input weight matrices to be estimated in a parallel manner.
\section{Alternating Direction Method of Multipliers}
Duh et al \cite{Duh2011} propose the use of \ac{ADMM} for the Learning-to-Rank task. \ac{ADMM} is a general optimization method that solves problems of the form
\begin{alignat*}{2}
\text{minimize }   &  f(x) + g(x) \\
\text{subject to } &  Ax + Bz = c
\end{alignat*}
by updating $x$ and $z$ in an alternating fashion. It holds the nice properties that it can be executed in parallel and that it allows for incremental model updates without full retraining. Duh et al \cite{Duh2011} show how to use \ac{ADMM} to train a RankSVM model in parallel. Experiments showed the \ac{ADMM} implementation to achieve a 13.1x training time speed-up on 72 workers, compared to training on a single worker.\\

Another \ac{ADMM} Learning-to-Rank based approach was proposed by Boyd et al \cite{Boyd2012}. Boyd et al \cite{Boyd2012} implemented an \ac{ADMM} based Learning-to-Rank method in Pregel \cite{Malewicz2010}, a parallel computation framework for graph computations. No experimental results on parallelisation speed-up were reported on this Pregel based approach.
\section{Distributed Hyper-parameter Tuning using MapReduce}
Ganjisaffar et al \cite{Ganjisaffar2011} observed that long training times are often a result of training many models for hyperparameter optimisation. Grid search is the \emph{de facto} standard of hyperparameter optimisation and is simply an exhaustive search through a manually specified subset of hyperparameter combinations. The authors show how to perform parallel grid search on MapReduce clusters, which is easy because grid search is an embarrassingly parallel method as hyperparameter combinations are mutually independent. They apply their grid search on MapReduce approach in a Learning-to-Rank setting to train a LambdaMART \cite{Wu2008} ranking model, a model that uses the Gradient Boosting \cite{Friedman2002} ensemble method combined with regression tree weak learners. Experiments showed that the solution scales linearly in number of hyperparameter combinations. However, the risk of overfitting grows overwhelmingly as the number of hyperparameter combinations grow, even when validation sets grow large.