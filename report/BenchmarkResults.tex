\chapter{Yahoo! Learning to Rank Challenge}
Yahoo's observation that all existing benchmark datasets were too small to draw reliable conclusions, especially in comparison with datasets used in commercial search engines, prompted Yahoo to release two internal datasets from Yahoo! search. The Yahoo! Learning to Rank Challenge\cite{Chapelle2011a} is a public Learning-to-Rank competition which took place from March to May 2010, with the goal to promote the datasets and encourage the research community to develop new Learning-to-Rank algorithms.\\

The Yahoo! Learning to Rank Challenge consists of two tracks that uses the two datasets respectively: a standard Learning-to-Rank track and a transfer learning track where the goal was to learn a specialized ranking function that can be used for a small country by leveraging a larger training set of another country. For this experiment I will only look at the standard Learning-to-Rank dataset because transfer learning is a separate research area that is not included in this thesis.\\
\begin{table}[!h]
\begin{tabular}{l|lll}
 & Train & Validation & Test \\ 
 \hline
Queries & 19,994 & 2,994 & 6,983 \\ 
Documents & 473,134 & 71,083 & 165,660 \\ 
Features & 519 & - & - \\ 
\end{tabular}
\caption{Yahoo! Learning to Rank Challenge dataset characteristics, as described in the challenge overview paper\cite{Chapelle2011a}}
\label{tab:yahoo_characteristics}
\end{table}\\
Both \ac{nDCG} and \ac{ERR} are measured as performance metrics, but the final standings of the challenge were based on the \ac{ERR} values. Model validation on the Learning-to-Rank methods participating in the challenge is performed using a train/validation/test-set split following the characteristics shown in Table \ref{tab:yahoo_characteristics}. Competitors could train on the training set and get immediate feedback on their performance on the validation set. The test set performance is used to create the final standings and is only measured after the competition has ended to avoid overfitting on the test set. The large number of documents, queries and features compared to other benchmark datasets makes the Yahoo! Learning to Rank Challenge dataset interesting. Yahoo did not provide detailed feature descriptions to prevent competitors to get detailed insight in the characteristics of the Yahoo data collection and features used at Yahoo. Instead high level descriptions of feature categories are provided. The following categories of features are described in the challenge overview paper\cite{Chapelle2011a}:\\
\begin{description}
\item[Web graph]{Quality and popularity metrics of web documents, e.g. PageRank\cite{Page1999}}.
\item[Document statistics]{Basic document statistics such as the number of words and url characteristics.}
\item[Document classifier]{Results of various classifiers on the documents. These classifiers amongst others include: spam, adult, language, main topic, and quality classifiers.}
\item[Query]{Basic query statistics, such as the number of terms, query frequency, and click-through rate.}
\item[Text match]{Textual similarity metrics between query and document. Includes \ac{TF-IDF}, BM25\cite{Robertson2009} and other metrics for different sections of the document.}
\item[Topical matching]{These features go beyond similarity at word level and compute similarity on topic level. For example by classifying both the document and the query in a large topical taxonomy.}
\item[Click]{Click-based user feedback.}
\item[External references]{Document meta-information such as Delicious\footnote{https://delicious.com/} tags}
\item[Time]{Document age and historical in- and outlink data that might help for time sensitive queries.}
\end{description}

\section{Results}
Figure \ref{fig:yahoo_results} shows the top five participants in the Yahoo! Learning to Rank Challenge in terms of ERR score. The top five participants all used decision trees and ensemble methods. Burges et al\cite{Burges2011} created a linear combination ensemble of eight LambdaMART\cite{Burges2010}, two LambdaRank and two Logistic Regression models. Gottschalk and Vogel used a combination of RandomForest models and \ac{GBDT} models. Pavlov and Brunk used a regression based model using the BagBoo\cite{Pavlov2010} ensemble technique, which combines bagging and boosting. Sorokina used a similar combination of bagging and boosting that is called Additive Groves\cite{Sorokina2007}.\\

The challenge overview paper states as one of the lessons of the challenge that the simple baseline \ac{GBDT} model performed very well with a small performance gap to the complex ensemble submissions at the top of the table.

\begin{table}
\begin{tabular}{l|p{6.3cm}|l}
 & Authors & ERR \\
 \hline 
1 & Burges et al (Microsoft Research) & 0.46861 \\ 
2 & Gottschalk (Activision Blizzard) \& Vogel (Data Mining Solutions) & 0.46786 \\ 
3 & Parakhin (Microsoft) & 0.46695 \\ 
4 & Pavlov \& Brunk (Yandex Labs) & 0.46678 \\ 
5 & Sorokina (Yandex Labs) & 0.46616 \\ 
\end{tabular}
\caption{Final standings of the Yahoo! Learning to Rank Challenge, as presented in the challenge overview paper\cite{Chapelle2011a}}
\label{fig:yahoo_results}
\end{table}



\chapter{Yandex Internet Mathematics competition}
\section{Benchmark characteristics}
\section{Results}



\chapter{LETOR}
The LETOR benchmark set was first released by Microsoft Research Asia in April 2007 to solve the absence of a experimental platform for Learning-to-Rank at that time. LETOR has been updated several times since: LETOR 2.0 was released at the end of 2007, LETOR 3.0 in December 2008 and LETOR 4.0. The original benchmark\cite{Liu2007b} as released in 2007 contained
\section{Benchmark characteristics}
\begin{table}[!h]
\scalebox{0.87}{
\begin{tabular}{p{0.29cm}|p{7.26cm}||p{0.29cm}|p{9.55cm}}
 \textbf{ID} & \textbf{Feature Description} & \textbf{ID} & \textbf{Feature Description}\\ 
 \hline
1 & \ac{TF} of body					& 36& LMIR.JM of body\\
2 &	\ac{TF} of anchor				& 37& LMIR.JM of anchor\\
3 & \ac{TF} of title				& 38& LMIR.JM of title\\
4 & \ac{TF} of \ac{URL}				& 39& LMIR.JM of \ac{URL}\\
5 & \ac{TF} of whole document		& 40& LMIR.JM of whole document\\

6 & \ac{IDF} of body 				& 41& Sitemap based term propagation\\
7 & \ac{IDF} of anchor 				& 42& Sitemap based score propagation\\
8 & \ac{IDF} of title 				& 43& Hyperlink based score propagation: weighted in-link\\
9 & \ac{IDF} of \ac{URL} 			& 44& Hyperlink based score propagation: weighted out-link\\
10& \ac{IDF} of whole document 		& 45& Hyperlink based score propagation: uniform out-link\\

11& \ac{TF-IDF} of body 			& 46& Hyperlink based feature propagation: weighted in-link\\
12& \ac{TF-IDF} of anchor 			& 47& Hyperlink based feature propagation: weighted out-link\\
13& \ac{TF-IDF} of title 			& 48& Hyperlink based feature propagation: uniform out-link\\
14& \ac{TF-IDF} of \ac{URL}			& 49& HITS authority\\
15& \ac{TF-IDF} of whole document	& 50& HITS hub\\

16& Document length of body			& 51& PageRank\\
17& Document length of anchor		& 52& HostRank\\
18& Document length of title		& 53& Topical PageRank\\
19& Document length of \ac{URL}		& 54& Topical HITS authority\\
20& Document length of whole document	& 55& Topical HITS hub\\

21& BM25 of body					& 56& In-link number\\
22& BM25 of anchor					& 57& Out-link number\\
23& BM25 of title					& 58& Number of slashes in \ac{URL}\\
24& BM25 of \ac{URL}				& 59& Length of \ac{URL}\\
25& BM25 of whole document			& 60& Number of child page\\

26& LMIR.ABS\cite{Zhai2001} of body	& 61& BM25 of extracted title\\
27& LMIR.ABS of anchor				& 62& LMIR.ABS of extracted title\\
28& LMIR.ABS of title				& 63& LMIR.DIR of extracted title\\
29& LMIR.ABS of \ac{URL}			& 64& LMIR.JM of extracted title\\
30& LMIR.ABS of whole document\\

31& LMIR.DIR of body\\
32& LMIR.DIR of anchor\\
33& LMIR.DIR of title\\
34& LMIR.DIR of \ac{URL}\\
35& LMIR.DIR of whole document\\
\end{tabular}
}
\caption{Features of the LETOR .GOV dataset, obtained from Qin et al\cite{Qin2010}}
\label{tbl:features_gov}
\end{table}
\section{Results}



\chapter{MSLR-WEB10k/MSLR-WEB30k}
Contrary to the Yahoo! Learning to Rank Challenge dataset, the MSLR-WEB30k provides detailed feature descriptions. The MSLR-WEB30k dataset however contains no proprietary features but only features that are commonly used in the research community.
\section{Benchmark characteristics}
\section{Results}

\chapter{Selecting Learning-to-Rank methods}
The Accuracies of the Learning-to-Rank methods described in the preceding chapters must only be compared within the benchmark and not between benchmarks for the following reasons:
\begin{enumerate}
\item Differences in feature sets between data sets detract from fair comparison
\item Although the \ac{nDCG} definition is unambiguous, Busa-Fekete et al\cite{Busa-Fekete2012} found that \ac{nDCG} evaluation tools of benchmark data sets produced different scores
\end{enumerate}
