[Slide 1]
Welcome everyone to the presentation of my master thesis research.
I am here to present to you my research with the title "Scaling Learning to Rank to Big Data", 
with the subtitle "Learning to Rank using MapReduce".

From this title you can deduct that it has two main topics: on the one hand it involves big data techniques, 
which, as its name suggests, are techniques to handle very large volumes of data.
The second main topic of this research is called "Learning to Rank". Learning to Rank is a research 
field that has been getting a lot of attention lately, that involves the use of Machine Learning techniques
 to predict rankings.

[Slide 2]
This is a typical, supervised, Machine Learning experiment set-up. We have a set of data, that can be described in
several attributes, or features, we have the labels of these data items. We apply a learning algorithm, we obtain
a predictive model, which we can now use to label any new data items.

[Slide 3]
Often this label is of categorical nature. An example of this are fraud detection models that are used at banks.
In this example financial transactions are our data items. Things like, sender, receiver, transaction amount might
be our featres in the feature vector, and our labels are either fraudulent or non-fraudulent. Based on historical
data for which it is known whether the transactions were fraudulent or not we can create our predictive model, 
which we can use real-time to detect fraudulent transactions.

[Slide 4]
Machine Learning models can also be used to solve the regression task instead of the classification task, in this
case we use our model to predict continuous scale values. Think for example about financial forecasts, where we 
want to have an actual number as our prediction. Recently the machine learning community has started to focus 
on a third type of Machine Learning task, called Learning to Rank, where the predictive model outputs rankings,
instead of a categorical of real valued prediction.

[Slide 5]
A well-known example of a field where Learning to Rank is applicable is in the world of search engines, Yahoo!, in
this example. What search engines do is they use attributes about the search query, attributes about the 
documents, and often also attributes about you, the person searching for information, as input features, to 
predict the most relevant ranking of documents. So Yahoo! here ranked documents to optimize relevance for the 
search term Avanade. 1,2,3,4,5.

[Slide 11]
So, how do we decide what the most relevant ranking is? Might a different ranking where the second and the third 
search result have been swapped be better perhaps? To decide which ranking is better, and also to be able to use
predictive models to optimize our rankings, we need a metric that quantifies the quality of a ranking.

[Slide 12]
There are multiple metrics that does this. But the one most frequently used in literature is NDCG, Normalized
Discounted Cumulative Gain. To be able to calculate NDCG we need a set of "ground truth" relevance labels that can
be obtained through experiments with human assessors, or alternatively they can be infered from click-through logs.
Cumulative Gain is based on the observation that, perhaps trivial, relevant documents tend to be more useful than
less relevant documents. We can calculate it by summing all the relevance labels of an ordered ranking of length p.
Discounted Normalized Gain adds to this the observation that relevant documents are more useful when the are 
higher up in the ranking. For each position in the ranking it calculated 2 to the power of the relevance, minus 1, 
divided by the 2log of i+1, and then sums over all positions in the ranking. Normalized Discounted Cumulative Gain 
calculates the DCG over the predicted ranking, and than divides that by the DCG that would have been obtained is 
it would have been calculated on the list of documents descendingly sorted on relevance.

[Slide 13]
An example. Here we have a ranking of length 10. The upper two rows represent the generated ranking that we want 
to evaluate, with the upper row being the relevance label and the second row being the DCG values.
The lower two rows are the best possible ranking of the same documents, again the upper row representing the 
relevance labels and the lower two rows representing their DCG values. When we sum the DCG values of the actual 
ranking and divide that by the sum of DCG values on the optimal ranking, we find that this ranking has a NDCG 
value of about 0.93 on a 0 to 1 scale, so it is actually quite good.

[Slide 14]
A lot of research has been done in the finding more accurate ranking methods. Recently, research teams at both 
Microsoft as well as Yahoo! identified applying Learning to Rank data sets on very large data sets to be an 
underexposed research area that is quickly gaining relevance as training data is quickly getting larger and 
larger. Based on this observation by Microsoft and Yahoo! our ultimate research goal is to find a way of 
processing large amounts of data with Learning to Rank algorithms. There are many ways to approach this problem,
one could focus on designing low time complexity learning algorithms, or on designing dedicated hardware for 
Learning to Rank. The approach that we take is to explore the speed-up that can be achieved when we execute a 
Learning to Rank algorithm in a distributed manner using MapReduce. We focus on MapReduce because it has over the 
years become the de facto method of distributed processing and therefore it is the distributed programming 
approach than can most easily be applied in practice. For other distributed programming approaches, expertise 
and organisational support is often lacking for other methods of distributed computation. So this is actually our 
second research question. Our first research question sets the scope for our second research question. There are 
many Learning to Rank algorithms, so which algorithm should we implement in MapReduce?

[Slide 15]
So for our first research question. There are many benchmark datasets that can be used to evaluate how good a 
Learning to Rank method is in terms of ranking accuracy. This slide shows the different existing Learning to Rank 
benchmark collections. Some of these are benchmark collections consisting of multiple benchmark datasets, so the 
total number of benchmark datasets is actually a multiple of these 8 benchmark collections. The problem with this 
is that there is no standard way of evaluating an algorithm. Many researchers have proposed new ranking methods 
and evaluated them on one or a couple of the benchmark datasets shown on this slide. But since some learning 
methods are evaluated on some benchmark sets and other learning methods are evaluated on other benchmark sets, 
it is hard to compare them. As an attempt to compare Learning to Rank algorithms across those different benchmark
datasets, I tried to collect all evaluation results of Learning to Rank methods on one of the benchmark datasets 
that are published in literature. My literature search strategy was as follows. Some of these benchmark 
collections have an accompanying paper to cite when you use the benchmark set. To gather evaluation results on
these benchmark collections we gathered all evaluation results published in papers that cited the benchmark papers.
For the other benchmark datasets that do not have an accompanying paper, we performed a google scholar search on 
the name of the dataset name.

[Slide 16]
Close to a thousand papers in total. Luckily a large share of those papers did not report any evaluation results 
on one of the benchmark sets, so these were filtered out. We chose to scope te comparison on NDCG values on 
ranking lengths of 3, 5 and 10, since these are ranking lengths often used when NDCG performance is reported. In 
addition we included evaluation results on another ranking metric, called Mean Average Precision. We did a couple
 of additional filtering steps on the set of papers, to filter out papers that used altered forms of benchmark 
sets, did not provide any exact results but instead only graphical representations. Also, each benchmark set 
comes with a few baseline ranking methods for which the creators of the set ran the evaluation. We also check 
whether the reported baseline performance was the same as the official baseline run, as any deviations from the 
official performance might indicate a difference in evaluation methodology.

[Slide 17]
This slide shows a table of the papers that survived our filtering steps. In total it includes evaluation results 
on 87 Learning to Rank methods.

[Slide 18]
The LETOR 3.0 benchmark collection consists of seven data sets and in the official LETOR 3.0 paper, the 
researchers compared eight learning to rank methods on those seven data sets. To make this comparison across 
multiple data sets they proposed a comparison methodology called Winning Number, which for each ranking algorithm 
counts the number of other ranking methods that it beats in terms of NDCG score, summed over all datasets.