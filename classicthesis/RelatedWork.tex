\chapter{Related Work}
\section{Search characteristics}
The literature research is performed by using the bibliographic databases Scopus and Web of Science with the following search query: \emph{("learning to rank" OR "learning-to-rank" OR "machine learned ranking") AND ("large scale" OR "parallel" OR "distributed")}. An abstract based manual filtering step is applied where I filter those results that actually use the \emph{large scale}, \emph{parallel} or \emph{distributed} terms in context to the \emph{learning to rank}, \emph{learning-to-rank} or \emph{machine learned ranking}. Studies focusing on efficient query evaluation instead of efficient model training are likely to meet all criteria listed. As a last step I will filter out studies focusing on efficient query evaluation.
\subsection{Scopus}
Defined search query resulted in 65 documents.
\subsection{Web of Science}
\section{CCRank}
Wang et al\cite{Wang2011a,Wang2011b} propose a parallel evolutionary algorithm based on \ac{CC}, a type of evolutionary algorithm. The \ac{CC} algorithm is capable of directly optimizing non-differentiable functions, as \ac{nDCG}, in contrary to many optimization algorithms.  the divide-and-conquer nature of the \ac{CC} algorithm enables parallelisation. CCRank showed an increase in both accuracy and efficiency on the LETOR 4.0 benchmark dataset compared to the baselines. It must be stated however that the increased efficiency was achieved through speed-up and not scale-up. Two reasons have been identified for not achieving linear scale-up with CCRank: 1) parallel execution is suspended after each generation to perform combination in order to produce the candidate solution, 2) Combination has to wait until all parallel tasks have finished, which may spend different running time.
\section{Parallel ListNet using Spark}
Shukla et al\cite{Shukla2012} explored the parallelisation of the well-known ListNet Learning-to-Rank method using Spark. Spark is a parallel computing model that is designed for cyclic data flows which makes it more suitable for iterative algorithms. Spark is incorporated into Hadoop since Hadoop 2.0. The Spark implementation of ListNet showed near a linear training time reduction.
\section{Gradient Boosted Distributed Decision Trees}
Ye et al\cite{Ye2009} described how to implement the \ac{GBDT} process in a parallel manner using both MPI and Hadoop. \ac{GBDT}'s are shown to be able to achieve good accuracy in a Learning-to-Rank setting when used in a pairwise\cite{Zheng2007} or listwise\cite{Wang2011} setting. Experiments showed the Hadoop implementation to result into too expensive communication cost to be useful. Authors believed that these high communication costs were a result of the communication intensive implementation that was not well suited for the MapReduce paradigm. The MPI approach proved to be successful and obtained near linear speed-ups.
\section{nDCG-Annealing}
Karimzadeghan el al\cite{Karimzadehgan2011} proposed a method using Simulated Annealing along with the Simplex method for its parameter search. This method directly optimises the often non-differentiable Learning-to-Rank evaluation metrics like \ac{nDCG} and \ac{MAP}. The authors successfully parallelised their method in the MapReduce paradigm using Hadoop. The approach showed to be effective on both the LETOR 3.0 dataset and their own dataset with contextual advertising data. Unfortunately their work does not directly report on the speed-up obtained by parallelising  with Hadoop, but it is mentioned that further work needs to be done to effectively leverage parallel execution.
\section{Low complexity learning}
Designing Learning-to-Rank algorithms with low time complexity for training is a different another approach towards large scale Learning-to-Rank. Pahikkala et al\cite{Pahikkala2009} describe a pairwise \ac{RLS} type of ranking function, RankRLS, with time complexity $\mathcal{O}(n^3+n^2m)$, with $n$ the number of features and $m$ the number of training documents. The RankRLS ranking function showed ranking performance similar to RankSVM on the BioInfer corpus\cite{Pyysalo2007}, a corpus for information extraction in the biomedical domain.
