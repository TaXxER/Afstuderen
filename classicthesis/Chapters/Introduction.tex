%********************************************************************
% Appendix
%*******************************************************
% If problems with the headers: get headings in appendix etc. right
%\markboth{\spacedlowsmallcaps{Appendix}}{\spacedlowsmallcaps{Appendix}}
\chapter{Motivation and Problem Statement}
Several Learning to Rank (LTR) benchmark datasets have been proposed in recent years \cite{Chapelle2011a,Qin2010,Alcantara2010} (Yandex Internet Mathematics competition). Chapelle et al \cite{Chapelle2011b} observed that the top competitors' results in LTR competitions are very close to each other, which raises the presumption that high accuracy LTR is a 'solved problem'.\\

Chappelle et al \cite{Chapelle2011b} identified efficiency and scalability of LTR algorithms to be one of the most important directions of future research in the field of LTR. The rationale behind this observation is that IR datasets are becoming bigger and bigger and that algorithm efficiency and scalability has not played a role in any LTR benchmark so far.\\

Several attempts on parallel or distributed machine learning \cite{Chu2007,Chang2007} has been made. The work on parallel or distributed LTR is still very limited compared to the amount of work that has been done on parallel or distributed regular machine learning. The field of efficient LTR has been getting some attention lately \cite{Asadi2013a,Asadi2013b,Busa-Fekete2012,Sousa2012,Shukla2012} since Chapelle at all \cite{Chapelle2011b} stated its growing importance back in 2011, only few of these studies \cite{Sousa2012,Shukla2012} have explored the possibilities of efficient LTR through the use of parallel programming paradigms. Lin \cite{Lin2013} observed that algorithms that are of iterative nature, like most LTR algorithms, are not amenable to the MapReduce framework. Lin \cite{Lin2013} argued that iterative algorithms can often be replaced with non-iterative alternatives or can still be optimized in such a way that its performance in a MapReduce setting is good enough.\\

\chapter{Research Goals}
The objective of this thesis is the exploration of execution time speed-up of Learning-to-Rank algorithms through parallelization using Hadoop. 
This work focuses on those Learning-to-Rank algorithms that have shown leading performance on relevant benchmark datasets.
The research questions raised and answered in this work include:
\begin{itemize}
\item What is the speed-up of existing Learning-to-Rank algorithms when implemented and executed on Hadoop?
\item Can we adjust those Learning-to-Rank algorithms such that the parallel execution speed-up increases without decreasing accuracy?
\end{itemize}

\chapter{Approach}
Equidem detraxit cu nam, vix eu delenit periculis. Eos ut vero
constituto, no vidit propriae complectitur sea. Diceret nonummy in
has, no qui eligendi recteque consetetur. Mel eu dictas suscipiantur,
et sed placerat oporteat. At ipsum electram mei, ad aeque atomorum
mea.

\chapter{Thesis Overview}

\begin{table}
    \myfloatalign
  \begin{tabularx}{\textwidth}{Xll} \toprule
    \tableheadline{labitur bonorum pri no} & \tableheadline{que vista}
    & \tableheadline{human} \\ \midrule
    fastidii ea ius & germano &  demonstratea \\
    suscipit instructior & titulo & personas \\
    %postulant quo & westeuropee & sanctificatec \\
    \midrule
    quaestio philosophia & facto & demonstrated \\
    %autem vulputate ex & parola & romanic \\
    %usu mucius iisque & studio & sanctificatef \\
    \bottomrule
  \end{tabularx}
  \caption[Autem usu id]{Autem usu id.}
  \label{tab:moreexample}
\end{table}

Ei solet nemore consectetuer nam. Ad eam porro impetus, te choro omnes
evertitur mel. Molestie conclusionemque vel at, no qui omittam
expetenda efficiendi. Eu quo nobis offendit, verterem scriptorem ne
vix.

  
\begin{lstlisting}[float,caption=A floating example]
for i:=maxint to 0 do
begin
{ do nothing }
end;
\end{lstlisting}