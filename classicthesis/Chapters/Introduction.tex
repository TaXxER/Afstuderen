%********************************************************************
% Appendix
%*******************************************************
% If problems with the headers: get headings in appendix etc. right
%\markboth{\spacedlowsmallcaps{Appendix}}{\spacedlowsmallcaps{Appendix}}
\chapter{Motivation and Problem Statement}
Several Learning to Rank (LTR) benchmark datasets have been proposed in recent years \cite{Chapelle2011a,Qin2010,Alcantara2010} (Yandex Internet Mathematics competition). Chapelle et al \cite{Chapelle2011b} observed that the top competitors' results in LTR competitions are very close to each other, which raises the presumption that high accuracy LTR is a 'solved problem'.\\

Chappelle et al \cite{Chapelle2011b} identified efficiency and scalability of LTR algorithms to be one of the most important directions of future research in the field of LTR. The rationale behind this observation is that IR datasets are becoming bigger and bigger and that algorithm efficiency and scalability has not played a role in any LTR benchmark so far.\\

Several attempts on parallel or distributed machine learning \cite{Chu2007,Chang2007} has been made. The work on parallel or distributed LTR is still very limited compared to the amount of work that has been done on parallel or distributed regular machine learning. The field of efficient LTR has been getting some attention lately \cite{Asadi2013a,Asadi2013b,Busa-Fekete2012,Sousa2012,Shukla2012} since Chapelle at all \cite{Chapelle2011b} stated its growing importance back in 2011, only few of these studies \cite{Sousa2012,Shukla2012} have explored the possibilities of efficient LTR through the use of parallel programming paradigms. Lin \cite{Lin2013} observed that algorithms that are of iterative nature, like most LTR algorithms, are not amenable to the MapReduce framework. Lin \cite{Lin2013} argued that iterative algorithms can often be replaced with non-iterative alternatives or can still be optimized in such a way that its performance in a MapReduce setting is good enough.\\

\chapter{Research Goals}
The objective of this thesis is the exploration of execution time speed-up of Learning-to-Rank algorithms through parallelization using Hadoop. 
This work focuses on those Learning-to-Rank algorithms that have shown leading performance on relevant benchmark datasets.
The research questions raised and answered in this work include:
\begin{itemize}
\item What is the speed-up of existing Learning-to-Rank algorithms when implemented and executed on Hadoop?
\item Can we adjust those Learning-to-Rank algorithms such that the parallel execution speed-up increases without decreasing accuracy?
\end{itemize}

\chapter{Approach}
Equidem detraxit cu nam, vix eu delenit periculis. Eos ut vero
constituto, no vidit propriae complectitur sea. Diceret nonummy in
has, no qui eligendi recteque consetetur. Mel eu dictas suscipiantur,
et sed placerat oporteat. At ipsum electram mei, ad aeque atomorum
mea.

\chapter{Thesis Overview}

\begin{description}
\item[Part II ]{introduces the reader to the basic principles and recent work in the fields of Learning-to-Rank}
\item[Part III ]{describes a short overview work in the field of parallel machine learning and Learning-to-Rank.}
\item[Part IV ]{describes implementation details of the Learning-to-Rank algorithms in the Hadoop framework.}
\item[Part V ]{presents and discusses speed-up results for the implemented Learning-to-Rank methods.}
\item[Part VI ]{summarizes the results. The limitations of our research as well as future research directions in the field are mentioned here.}
\end{description}